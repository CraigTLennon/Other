{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, the Markov property is:\n",
    "$$\n",
    "p(s^\\prime,r \\, |\\, s,a):= p \\left(S_{t+1}, R_{t+1}\\, | \\,S_i, A_i   \\right)=p \\left( S_{t+1}, R_{t+1} \\, | \\, S_i, A_i \\forall i\\leq t \\right)\n",
    "$$\n",
    "marginals can be found by summing over the non-marginalized variable\n",
    "$$\n",
    "p(s^\\prime \\, | \\, s,a) = \\sum_{r \\in R} p(s^\\prime,r \\, |\\, s,a), \\qquad\n",
    "p(r\\, | \\, s,a) = \\sum_{s^\\prime \\in S} p(s^\\prime,r \\, |\\, s,a)\n",
    "$$\n",
    "The markov decision process consists of 5 things:\n",
    "\n",
    "Set of states\n",
    "\n",
    "Set of actions\n",
    "\n",
    "Set of rewards\n",
    "\n",
    "State-transition probabilites and reward probabilities (defined jointly)\n",
    "\n",
    "Discount factor\n",
    "\n",
    "Written as 5-tuple (S,A,R,P,D)\n",
    "\n",
    "Decisons are made with a policy $\\pi$, which is the method by which agent navigates the environment\n",
    "\n",
    "The transitions are stochastic because the state is an imperfect representation of the environment\n",
    "\n",
    "Total future reward (which guides agent actions and does not include current reward) is represented by G:\n",
    "$$\n",
    "G(t) = \\sum_{\\tau=1}^\\infty d(\\tau)R (t+\\tau) \\textrm{ in these examples we have } d(\\tau)=\\gamma^{\\tau-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
