{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods \n",
    "We will be using sqaured error as our cost function.\n",
    "\n",
    "$$\n",
    "V(s) \\leftarrow  V(s) + \\alpha \\left( G_s - V(s) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN is simple.  Once a metric is chosen, you examine all points in the training set to determine which k are closest to the test point.  The choose the label most prevelent among those k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sortedcontainers import SortedList\n",
    "\n",
    "\n",
    "\n",
    "#Defining the class to have fit and predict functions like scikit-learn\n",
    "class KNN(object):\n",
    "    def __init__(self,k):\n",
    "        self.k=k\n",
    "        \n",
    "    def fit(self,x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        \n",
    "    def predict(self,Xtest):\n",
    "        y=np.zeros(len(Xtest))\n",
    "        for i,x in enumerate(Xtest): #returns a count and object of the iterable\n",
    "            sl=SortedList(load=self.k) #from sorted containers, a sorted list that will make it easy\n",
    "            #to track distances\n",
    "            for j,xt in enumerate(self.x): #this is training X\n",
    "                diff=x-xt\n",
    "                d=diff.dot(diff) #dot product\n",
    "                if len(sl)<self.k:\n",
    "                    sl.add((d,self.y[j])) #add the distance and the class\n",
    "                else:\n",
    "                    if d<sl[-1][0]:\n",
    "                        del sl[-1]\n",
    "                        sl.add((d,self.y[j])) \n",
    "                #at the end of this for loop we have the k closest elements\n",
    "            votes={}\n",
    "            for _,v in sl: # v is the vote for a class\n",
    "                votes[v]=votes.get(v,0)+1\n",
    "                max_vote=0\n",
    "                max_class=-1\n",
    "                for v,count in votes.items():\n",
    "                    if count >max_vote:\n",
    "                        max_vote=count\n",
    "                        max_class=v\n",
    "                        \n",
    "                y[i]=max_class\n",
    "        return(y)\n",
    "                \n",
    "    def score(self,x,y):\n",
    "        p=self.predict(x)\n",
    "\n",
    "        return( np.mean(p==y))\n",
    "            \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder=\"C:/Users/craig_arl/Documents/GitHub/machine_learning_examples/mnist_csv/\"\n",
    "xtrn=data_folder+\"Xtrain.txt\"\n",
    "xtest=data_folder+\"Xtest.txt\"\n",
    "ytrn=data_folder+\"label_train.txt\"\n",
    "ytest=data_folder+\"label_test.txt\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "def get_data(f_name_X,f_name_Y,limit=None):\n",
    "    print(\"Reading in and transforming data...\")\n",
    "    df_x = pd.read_csv(f_name_X)\n",
    "    df_y = pd.read_csv(f_name_Y)\n",
    "    df=df_y.join(df_x)\n",
    "   \n",
    "    data = df.as_matrix()\n",
    "    np.random.shuffle(data)\n",
    "    X = data[:, 1:] / 255.0 # data is from 0..255\n",
    "    Y = data[:, 0]\n",
    "    if limit is not None:\n",
    "        X, Y = X[:limit], Y[:limit]\n",
    "    return X, Y\n",
    "\n",
    "#copied:\n",
    "def get_xor():\n",
    "    X = np.zeros((200, 2))\n",
    "    X[:50] = np.random.random((50, 2)) / 2 + 0.5 # (0.5-1, 0.5-1)\n",
    "    X[50:100] = np.random.random((50, 2)) / 2 # (0-0.5, 0-0.5)\n",
    "    X[100:150] = np.random.random((50, 2)) / 2 + np.array([[0, 0.5]]) # (0-0.5, 0.5-1)\n",
    "    X[150:] = np.random.random((50, 2)) / 2 + np.array([[0.5, 0]]) # (0.5-1, 0-0.5)\n",
    "    Y = np.array([0]*100 + [1]*100)\n",
    "    return X, Y\n",
    "\n",
    "def get_donut():\n",
    "    N = 200\n",
    "    R_inner = 5\n",
    "    R_outer = 10\n",
    "\n",
    "    # distance from origin is radius + random normal\n",
    "    # angle theta is uniformly distributed between (0, 2pi)\n",
    "    R1 = np.random.randn(N//2) + R_inner\n",
    "    theta = 2*np.pi*np.random.random(N//2)\n",
    "    X_inner = np.concatenate([[R1 * np.cos(theta)], [R1 * np.sin(theta)]]).T\n",
    "\n",
    "    R2 = np.random.randn(N//2) + R_outer\n",
    "    theta = 2*np.pi*np.random.random(N//2)\n",
    "    X_outer = np.concatenate([[R2 * np.cos(theta)], [R2 * np.sin(theta)]]).T\n",
    "\n",
    "    X = np.concatenate([ X_inner, X_outer ])\n",
    "    Y = np.array([0]*(N//2) + [1]*(N//2))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in and transforming data...\n",
      "Reading in and transforming data...\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train=get_data(xtrn,ytrn)\n",
    "x_test,y_test=get_data(xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=KNN(5)\n",
    "knn.fit(x_train,y_train)\n",
    "knn.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm \n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "class NaiveBayes(object):\n",
    "    def fit(self, x,y,smoothing=10e-3):\n",
    "        self.gaussians=dict()\n",
    "        self.priors=dict()\n",
    "        labels=set(y.astype(int))\n",
    "        \n",
    "        for c in labels:\n",
    "            current_x=x[y==c]\n",
    "            self.gaussians[c]={'mean':current_x.mean(axis=0),\n",
    "                              'var': current_x.var(axis=0)+smoothing} #axis 0 down 1 across\n",
    "            self.priors[c]=float(len(y[y==c]))/len(y)\n",
    "    def score(self,x,y):\n",
    "        p=self.predict(x)\n",
    "        return( np.mean(p==y))\n",
    "    \n",
    "    def predict(self,x):\n",
    "        N,D=x.shape\n",
    "        K=len(self.gaussians)\n",
    "        P=np.zeros((N,K)) #matrix for the probabilities of each class\n",
    "        for c,g in self.gaussians.items():\n",
    "         \n",
    "            mean,var = g['mean'],g['var']\n",
    "            P[:,c]=mvn.logpdf(x,mean=mean,cov=var)+np.log(self.priors[c])   #for each x prob it is class c\n",
    "        return(np.argmax(P,axis=1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in and transforming data...\n",
      "Reading in and transforming data...\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train=get_data(xtrn,ytrn)\n",
    "x_test,y_test=get_data(xtest,ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71254250850170031"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=NaiveBayes()\n",
    "model.fit(x_train,y_train)\n",
    "model.score(x_test,y_test)\n",
    "model.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generic (non-naive) bayesian classifier that removes the conditional independence assumption.  In the code, we replace the variance by the covariance in fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm \n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "class Bayes(object):\n",
    "    \n",
    "    \n",
    "    def fit(self, x,y,smoothing=10e-3):\n",
    "        N,D=x.shape\n",
    "        print(N,D)\n",
    "        self.gaussians=dict()\n",
    "        self.priors=dict()\n",
    "        labels=set(y.astype(int))\n",
    "        \n",
    "        for c in labels:\n",
    "            current_x=x[y==c]\n",
    "            self.gaussians[c]={'mean':current_x.mean(axis=0),\n",
    "                              'cov': np.cov(current_x.T)+np.eye(D)*smoothing} #axis 0 down 1 across\n",
    "          \n",
    "            self.priors[c]=float(len(y[y==c]))/len(y)\n",
    "            \n",
    "            \n",
    "    def score(self,x,y):\n",
    "        p=self.predict(x)\n",
    "        return( np.mean(p==y))\n",
    "    \n",
    "    def predict(self,x):\n",
    "        N,D=x.shape\n",
    "        K=len(self.gaussians)\n",
    "        P=np.zeros((N,K)) #matrix for the probabilities of each class\n",
    "        for c,g in self.gaussians.items():\n",
    "         \n",
    "            mean,cov = g['mean'],g['cov']\n",
    "            P[:,c]=mvn.logpdf(x,mean=mean,cov=cov)+np.log(self.priors[c])   #for each x prob it is class c\n",
    "        return(np.argmax(P,axis=1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.71342685370741488"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelb=Bayes()\n",
    "modelb.fit(x_train,y_train)\n",
    "modelb.score(x_test,y_test)\n",
    "#modelb.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn1=np.array([-1,2,3])\n",
    "mn2=np.array([1,-1,3.5])\n",
    "mn3=np.array([0,2,-1])\n",
    "cov=np.matrix('1,-.2,.35;-.2,1.4,1;.35,1,1.3')\n",
    "cov2=np.matrix('1.9,.2,.35;.2,3.4,1;.35,1,2.3')\n",
    "rv=mvn.rvs(mn,cov) #generates a random sample\n",
    "X1=np.random.multivariate_normal(mn1,cov,1000)\n",
    "X2=np.random.multivariate_normal(mn2,cov,1000)\n",
    "X3=np.random.multivariate_normal(mn3,cov2,1000)\n",
    "y1=np.zeros(1000)\n",
    "y2=np.ones(1000)\n",
    "y3=np.ones(1000)*2\n",
    "\n",
    "X=np.concatenate((X1,X2,X3),axis=0)\n",
    "X.shape\n",
    "Y=np.concatenate((y1,y2,y3),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93166666666666664"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=NaiveBayes()\n",
    "model.fit(X,Y)\n",
    "model.score(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98133333333333328"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=Bayes()\n",
    "model.fit(X,Y)\n",
    "model.score(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
