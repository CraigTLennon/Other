{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dynamic programming, we had to know all the states, put the agent into a state, know transition prob etc.. Great solution technique when you know everything.  MC methods learn only from experience.  In RL, the random component is the return.  Instead of calculating the expectation of G, we estimate with sample mean.  Only works for episodic tasks because the episode must terminate before we can calculate rewards.  Methods are similar to multi-armed bandit, like each state in MABandit.  Start with prediction then move to control.\n",
    "$$\n",
    "V_\\pi(s) \\approx \\frac{1}{N} \\sum_i G_{i,s}\n",
    "$$\n",
    "We generate $G$ by playing a bunch of episodes, logging states and rewards.  We calculate $G$ by\n",
    "$$\n",
    "G(t)= r(t+1)+\\gamma G(t+1),\n",
    "$$\n",
    "iterating through states in reverse order, and once we have all (s,G) pairs we average for each s.\n",
    "\n",
    "\n",
    "What happens if you visit a state more than once (e.g. $t=1,3$).  Either you can use first visit ($G(1)$ only), or every visit ($G(1),G(3)$) and both lead to the same answer for large N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because we are using a sample mean the central limit theorem still applies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Grid: # Environment\n",
    "    def __init__(self, width, height, start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "\n",
    "    def set(self, rewards, actions):\n",
    "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "\n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "\n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "\n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "\n",
    "    def move(self, action):\n",
    "        # check if legal move first\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # return a reward (if any)\n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "\n",
    "    def undo_move(self, action):\n",
    "    # these are the opposite of what U/D/L/R should normally do\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # raise an exception if we arrive somewhere we shouldn't be\n",
    "        # should never happen\n",
    "        assert(self.current_state() in self.all_states())\n",
    "\n",
    "    def game_over(self):\n",
    "        # returns true if game is over, else false\n",
    "        # true if we are in a state where no actions are possible\n",
    "        return (self.i, self.j) not in self.actions\n",
    "\n",
    "    def all_states(self):\n",
    "        # possibly buggy but simple way to get all states\n",
    "        # either a position that has possible next actions\n",
    "        # or a position that yields a reward\n",
    "        return set(list(self.actions.keys()) + list(self.rewards.keys()))\n",
    "\n",
    "\n",
    "def standard_grid():\n",
    "    # define a grid that describes the reward for arriving at each state\n",
    "    # and possible actions at each state\n",
    "    # the grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .  .  .  1\n",
    "    # .  x  . -1\n",
    "    # s  .  .  .\n",
    "    g = Grid(3, 4, (2, 0))\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    actions = {\n",
    "        (0, 0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('L', 'D', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('L', 'R', 'U'),\n",
    "        (2, 3): ('L', 'U'),\n",
    "      }\n",
    "    g.set(rewards, actions)\n",
    "    return g\n",
    "\n",
    "\n",
    "def negative_grid(step_cost=-0.1):\n",
    "    # in this game we want to try to minimize the number of moves\n",
    "    # so we will penalize every move\n",
    "    g = standard_grid()\n",
    "    g.rewards.update({\n",
    "    (0, 0): step_cost,\n",
    "    (0, 1): step_cost,\n",
    "    (0, 2): step_cost,\n",
    "    (1, 0): step_cost,\n",
    "    (1, 2): step_cost,\n",
    "    (2, 0): step_cost,\n",
    "    (2, 1): step_cost,\n",
    "    (2, 2): step_cost,\n",
    "    (2, 3): step_cost,\n",
    "    })\n",
    "    return g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SMALL_ENOUGH=10**-4\n",
    "def print_values(V,g):\n",
    "    for i in range(g.width):\n",
    "        print(\"-------------------------\")\n",
    "        for j in range(g.height):\n",
    "            v=V.get((i,j),0)\n",
    "            if v>=0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "def print_policy(P,g):\n",
    "    for i in range(g.width):\n",
    "        print(\"\")\n",
    "        print(\"----------------\")\n",
    "        for j in range(g.height):\n",
    "            p=P.get((i,j),' ')\n",
    "            print(\" %s |\" % p,end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_POSSIBLE_ACTIONS=('U','D','L','R')\n",
    "GAMMA=0.9\n",
    "#policy evaluation with MC\n",
    "def play_game(grid,policy):\n",
    "    #return states and returns\n",
    "    #reset to start at random posn,\n",
    "    \n",
    "    start_states=list(grid.actions.keys())\n",
    "    start_idx=np.random.choice(len(start_states))\n",
    "    grid.set_state(start_states[start_idx])\n",
    "    \n",
    "    s=grid.current_state()\n",
    "    state_reward=[(s,0)] #state reward tuple\n",
    "    while not grid.game_over():\n",
    "        a=policy[s]\n",
    "        r=grid.move(a)\n",
    "        s=grid.current_state()\n",
    "        state_reward.append((s,r))\n",
    "    G=0\n",
    "    state_return=[]\n",
    "    first = True\n",
    "#     print(state_reward)\n",
    "    for s,r in reversed(state_reward):\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            #ignore first state bc value for terminal state is 0\n",
    "            state_return.append((s,G))\n",
    "        G=r+GAMMA*G\n",
    "#     print(state_return)\n",
    "    state_return.reverse()\n",
    "    \n",
    "    return state_return\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "-------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "-------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "-------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "values\n",
      "\n",
      "-------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "-------------------------\n",
      " 0.73| 0.00|-1.00| 0.00|\n",
      "-------------------------\n",
      " 0.66|-0.81|-0.90|-1.00|\n",
      "policy\n",
      "\n",
      "----------------\n",
      " R | R | R |   |\n",
      "----------------\n",
      " U |   | R |   |\n",
      "----------------\n",
      " U | R | R | U |"
     ]
    }
   ],
   "source": [
    "grid = standard_grid()\n",
    "\n",
    "# print rewards\n",
    "print(\"rewards:\")\n",
    "print_values(grid.rewards, grid)\n",
    "\n",
    "# state -> action\n",
    "policy = {\n",
    "(2, 0): 'U',\n",
    "(1, 0): 'U',\n",
    "(0, 0): 'R',\n",
    "(0, 1): 'R',\n",
    "(0, 2): 'R',\n",
    "(1, 2): 'R',\n",
    "(2, 1): 'R',\n",
    "(2, 2): 'R',\n",
    "(2, 3): 'U',\n",
    "}\n",
    "\n",
    "V = {}\n",
    "returns = {} # dictionary of state -> list of returns we've received\n",
    "states = grid.all_states()\n",
    "for s in states:\n",
    "    if s in grid.actions.keys():\n",
    "        returns[s] = []\n",
    "    else:\n",
    "  # terminal state or state we can't otherwise get to\n",
    "        V[s] = 0\n",
    "#now that everything is initialized we start the process of going through states\n",
    "for t in range(100):\n",
    "    #generate episode\n",
    "    state_return=play_game(grid,policy)\n",
    "#     print(state_return)\n",
    "    seen_states=set()\n",
    "    for s,G in state_return:\n",
    "        if s not in seen_states:\n",
    "            returns[s].append(G)\n",
    "            V[s]=np.mean(returns[s]) #just one thing but will be more later?\n",
    "            seen_states.add(s)\n",
    "            \n",
    "print(\"values\")\n",
    "print(\"\")\n",
    "print_values(V,grid)\n",
    "print(\"policy\")\n",
    "print_policy(policy,grid)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "-------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "-------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "-------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "values\n",
      "\n",
      "-------------------------\n",
      " 0.43| 0.56| 0.72| 0.00|\n",
      "-------------------------\n",
      " 0.33| 0.00| 0.21| 0.00|\n",
      "-------------------------\n",
      " 0.25| 0.19| 0.12|-0.15|\n",
      "policy\n",
      "\n",
      "----------------\n",
      " R | R | R |   |\n",
      "----------------\n",
      " U |   | U |   |\n",
      "----------------\n",
      " U | L | U | L |"
     ]
    }
   ],
   "source": [
    "#windy gridworld\n",
    "\n",
    "grid = standard_grid()\n",
    "def random_action(a):\n",
    "    p=np.random.random()\n",
    "    if p<.5:\n",
    "        return a\n",
    "    else:\n",
    "        tmp=list(ALL_POSSIBLE_ACTIONS)\n",
    "        tmp.remove(a)\n",
    "        return np.random.choice(tmp)\n",
    "    \n",
    "\n",
    "ALL_POSSIBLE_ACTIONS=('U','D','L','R')\n",
    "GAMMA=0.9\n",
    "#policy evaluation with MC\n",
    "def play_game(grid,policy):\n",
    "    #return states and returns\n",
    "    #reset to start at random posn,\n",
    "    \n",
    "    start_states=list(grid.actions.keys())\n",
    "    start_idx=np.random.choice(len(start_states))\n",
    "    grid.set_state(start_states[start_idx])\n",
    "    \n",
    "    s=grid.current_state()\n",
    "    state_reward=[(s,0)] #state reward tuple\n",
    "    while not grid.game_over():\n",
    "        a=policy[s]\n",
    "        a=random_action(a) #this is the only difference from the last one\n",
    "        r=grid.move(a)\n",
    "        s=grid.current_state()\n",
    "        state_reward.append((s,r))\n",
    "    G=0\n",
    "    state_return=[]\n",
    "    first = True\n",
    "#     print(state_reward)\n",
    "    for s,r in reversed(state_reward):\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            #ignore first state bc value for terminal state is 0\n",
    "            state_return.append((s,G))\n",
    "        G=r+GAMMA*G\n",
    "#     print(state_return)\n",
    "    state_return.reverse()\n",
    "    \n",
    "    return state_return\n",
    "\n",
    "        \n",
    "    \n",
    "# print rewards\n",
    "print(\"rewards:\")\n",
    "print_values(grid.rewards, grid)\n",
    "\n",
    "# state -> action\n",
    "policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'U',\n",
    "    (2, 1): 'L',\n",
    "    (2, 2): 'U',\n",
    "    (2, 3): 'L',\n",
    "  }\n",
    "\n",
    "V = {}\n",
    "returns = {} # dictionary of state -> list of returns we've received\n",
    "states = grid.all_states()\n",
    "for s in states:\n",
    "    if s in grid.actions.keys():\n",
    "        returns[s] = []\n",
    "    else:\n",
    "  # terminal state or state we can't otherwise get to\n",
    "        V[s] = 0\n",
    "#now that everything is initialized we start the process of going through states\n",
    "for t in range(5000):\n",
    "    #generate episode\n",
    "    state_return=play_game(grid,policy)\n",
    "#     print(state_return)\n",
    "    seen_states=set()\n",
    "    for s,G in state_return:\n",
    "        if s not in seen_states:\n",
    "            returns[s].append(G)\n",
    "            V[s]=np.mean(returns[s]) #just one thing but will be more later?\n",
    "            seen_states.add(s)\n",
    "            \n",
    "print(\"values\")\n",
    "print(\"\")\n",
    "print_values(V,grid)\n",
    "print(\"policy\")\n",
    "print_policy(policy,grid)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC for the control problem \n",
    "\n",
    "Key is to choose $\\text{argmax}_a Q(s,a),$ returning triples (s,a,G), which means we need $|S|\\times |A|$ iterations instead of just $|S|,$ so we need many more iterations of MC.  Further, with a fixed policy we only take one action per state.  We can fix this by using the \"exploring starts\" method, in which we choose a random initial state and a random initial action, thereafter following policy.  This makes sense given our definition of Q\n",
    "$$\n",
    "Q_\\pi(s,a)=E_\\pi \\left[ G(t) \\, | \\, S_t=s, A_t=a\\right]\n",
    "$$\n",
    "Then we deal with the control problem by alternating between policy evalution and greedy policy improvement. The improvement is the same as before:\n",
    "$$\n",
    "\\pi(s) = \\text{argmax}_a Q(s,a)\n",
    "$$\n",
    "\n",
    "Like in value iteration, we do not start a fresh MC evaluation each round, but instead keep updating the same Q, doing policy improvement after each episode.\n",
    "One side thing, we avoid getting stuck in a state (bumping into wall) by giving rewward of -100 is in same action and end episode.  Although not formally proven, we achieve stability when both value and policy converge to optimal value and optimal policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_POSSIBLE_ACTIONS=('U','D','L','R')\n",
    "GAMMA=0.9\n",
    "#policy evaluation with MC\n",
    "def play_game_es(grid,policy):\n",
    "    #return states and returns\n",
    "    #reset to start at random posn,\n",
    "    \n",
    "    start_states=list(grid.actions.keys())\n",
    "    start_idx=np.random.choice(len(start_states))\n",
    "    grid.set_state(start_states[start_idx])\n",
    "    \n",
    "    s=grid.current_state()\n",
    "    a=np.random.choice(ALL_POSSIBLE_ACTIONS) #THIS IS WHAT MAKES IT EXPLORING START\n",
    "    seen_states=set()\n",
    "    state_action_reward=[(s,a,0)] #state reward tuple\n",
    "    while True:\n",
    "        old_s=grid.current_state()\n",
    "#         print(old_s,a)\n",
    "        r=grid.move(a)\n",
    "        s=grid.current_state()\n",
    "        if s in seen_states:\n",
    "            state_action_reward.append((s,None,-100))\n",
    "            break\n",
    "        elif grid.game_over():\n",
    "            state_action_reward.append((s,None,r))\n",
    "            break\n",
    "        else:\n",
    "            a=policy[s]\n",
    "            state_action_reward.append((s,a,r))\n",
    "        seen_states.add(s)\n",
    "    G=0\n",
    "    state_action_return=[]\n",
    "    first = True\n",
    "#     print(state_reward)\n",
    "    for s,a,r in reversed(state_action_reward):\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            #ignore first state bc value for terminal state is 0\n",
    "            state_action_return.append((s,a,G))\n",
    "        G=r+GAMMA*G\n",
    "#     print(state_return)\n",
    "    state_action_return.reverse()\n",
    "    \n",
    "    return state_action_return\n",
    "\n",
    "def max_dict(d): #just a helper function for getting max from dict\n",
    "    max_key=None\n",
    "    max_val = float('-inf')\n",
    "    for k,v in d.items():\n",
    "        if v>max_val:\n",
    "            max_val=v\n",
    "            max_key=k\n",
    "    return max_key,max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "-------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "-------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "-------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "values\n",
      "\n",
      "-------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "-------------------------\n",
      " 0.42| 0.00| 0.80| 0.00|\n",
      "-------------------------\n",
      " 0.25| 0.46| 0.62| 0.46|\n",
      "policy\n",
      "\n",
      "----------------\n",
      " R | R | R |   |\n",
      "----------------\n",
      " U |   | U |   |\n",
      "----------------\n",
      " R | R | U | L |"
     ]
    }
   ],
   "source": [
    "grid = negative_grid()\n",
    "#print rewards\n",
    "print(\"rewards:\")\n",
    "print_values(grid.rewards, grid)\n",
    "\n",
    "policy={}\n",
    "for s in grid.actions.keys():\n",
    "    policy[s]=np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "    \n",
    "Q={}\n",
    "returns={}\n",
    "states = grid.all_states()\n",
    "#this is just initialization\n",
    "for s in states:\n",
    "    if s in grid.actions.keys():\n",
    "        Q[s]={}\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "            Q[s][a]=0\n",
    "            returns[(s,a)]=[]\n",
    "    else:\n",
    "        pass #terminal state or state cannot reach\n",
    "    \n",
    "#Now we start\n",
    "deltas=[] #for debugging, but could be used to test for convergence\n",
    "for t in range(20000):\n",
    "    if t % 1000 ==0:\n",
    "        print(t)\n",
    "        #generate episode using current policy\n",
    "    biggest_change=0\n",
    "    state_action_return=play_game_es(grid,policy)\n",
    "    seen_SAR=set()\n",
    "    #update samples of return function\n",
    "    for (s,a,G) in state_action_return:\n",
    "        #check if seen bc using first-visit policy\n",
    "        sa=(s,a)\n",
    "        if sa not in seen_SAR:\n",
    "            old_q=Q[s][a]\n",
    "            returns[sa].append(G)\n",
    "            Q[s][a]=np.mean(returns[sa])\n",
    "            biggest_change=max(biggest_change,abs(old_q-Q[s][a]))\n",
    "            seen_SAR.add(sa)\n",
    "    deltas.append(biggest_change)\n",
    "    \n",
    "    #now update policy\n",
    "    \n",
    "    for s in policy.keys():\n",
    "        policy[s]=max_dict(Q[s])[0] #taking the action with max value\n",
    "        \n",
    "#now find values V\n",
    "V={}\n",
    "for s, Qs in Q.items():\n",
    "    V[s]=max_dict(Q[s])[1] #get value of best action\n",
    "    \n",
    "\n",
    "print(\"values\")\n",
    "print(\"\")\n",
    "print_values(V,grid)\n",
    "print(\"policy\")\n",
    "print_policy(policy,grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning wthout exploring starts, which might not work unless you can control which state you are in \"in god mode.\"  In this case, we will be epsilon greedy instead of greedy.  Thus we ensure that every policy has a chance of being selected $\\pi(a|s)\\geq \\epsilon |A(s)|$. Many iterations are required to find states far from te policy (e.g. $\\left(\\epsilon/|A(s)|\\right)^k$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_POSSIBLE_ACTIONS=('U','D','L','R')\n",
    "GAMMA=0.9\n",
    "def random_action(a,eps=.1):\n",
    "    p=np.random.random()\n",
    "    if p<(1-eps):\n",
    "        return a\n",
    "    else:\n",
    "        return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "    \n",
    "\n",
    "\n",
    "#policy evaluation with MC\n",
    "def play_game_no_es(grid,policy):\n",
    "    #return states and returns\n",
    "    #reset to start at random posn,\n",
    "    s=(2,0)\n",
    "    grid.set_state(s)\n",
    "    a=random_action(policy[s])\n",
    "    state_action_reward=[(s,a,0)] #state reward tuple\n",
    "    while True:\n",
    "        r=grid.move(a)\n",
    "        s=grid.current_state()\n",
    "        if grid.game_over():\n",
    "            state_action_reward.append((s,None,r))\n",
    "            break\n",
    "        else:\n",
    "            a=random_action(policy[s])\n",
    "            state_action_reward.append((s,a,r))\n",
    "    G=0\n",
    "    state_action_return=[]\n",
    "    first = True\n",
    "#     print(state_reward)\n",
    "    for s,a,r in reversed(state_action_reward):\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            #ignore first state bc value for terminal state is 0\n",
    "            state_action_return.append((s,a,G))\n",
    "        G=r+GAMMA*G\n",
    "#     print(state_return)\n",
    "    state_action_return.reverse()\n",
    "    \n",
    "    return state_action_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "-------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "-------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "-------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "values\n",
      "\n",
      "-------------------------\n",
      " 0.54| 0.77| 1.00| 0.00|\n",
      "-------------------------\n",
      " 0.24| 0.00| 0.77| 0.00|\n",
      "-------------------------\n",
      " 0.22| 0.37| 0.54| 0.37|\n",
      "policy\n",
      "\n",
      "----------------\n",
      " R | R | R |   |\n",
      "----------------\n",
      " U |   | U |   |\n",
      "----------------\n",
      " R | R | U | L |"
     ]
    }
   ],
   "source": [
    "grid = negative_grid()\n",
    "#print rewards\n",
    "print(\"rewards:\")\n",
    "print_values(grid.rewards, grid)\n",
    "\n",
    "policy={}\n",
    "for s in grid.actions.keys():\n",
    "    policy[s]=np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "    \n",
    "Q={}\n",
    "returns={}\n",
    "states = grid.all_states()\n",
    "#this is just initialization\n",
    "for s in states:\n",
    "    if s in grid.actions.keys():\n",
    "        Q[s]={}\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "            Q[s][a]=0\n",
    "            returns[(s,a)]=[]\n",
    "    else:\n",
    "        pass #terminal state or state cannot reach\n",
    "    \n",
    "#Now we start\n",
    "deltas=[] #for debugging, but could be used to test for convergence\n",
    "for t in range(5000):\n",
    "    if t % 1000 ==0:\n",
    "        print(t)\n",
    "        #generate episode using current policy\n",
    "    biggest_change=0\n",
    "    state_action_return=play_game_no_es(grid,policy)\n",
    "    seen_SAR=set()\n",
    "    #update samples of return function\n",
    "    for (s,a,G) in state_action_return:\n",
    "        #check if seen bc using first-visit policy\n",
    "        sa=(s,a)\n",
    "        if sa not in seen_SAR:\n",
    "            old_q=Q[s][a]\n",
    "            returns[sa].append(G)\n",
    "            Q[s][a]=np.mean(returns[sa])\n",
    "            biggest_change=max(biggest_change,abs(old_q-Q[s][a]))\n",
    "            seen_SAR.add(sa)\n",
    "    deltas.append(biggest_change)\n",
    "    \n",
    "    #now update policy\n",
    "    \n",
    "    for s in policy.keys():\n",
    "        policy[s]=max_dict(Q[s])[0] #taking the action with max value\n",
    "        \n",
    "#now find values V\n",
    "V={}\n",
    "for s, Qs in Q.items():\n",
    "    V[s]=max_dict(Q[s])[1] #get value of best action\n",
    "    \n",
    "\n",
    "print(\"values\")\n",
    "print(\"\")\n",
    "print_values(V,grid)\n",
    "print(\"policy\")\n",
    "print_policy(policy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
