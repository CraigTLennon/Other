{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, the Markov property is:\n",
    "$$\n",
    "p(s^\\prime,r \\, |\\, s,a):= p \\left(S_{t+1}, R_{t+1}\\, | \\,S_i, A_i   \\right)=p \\left( S_{t+1}, R_{t+1} \\, | \\, S_i, A_i \\forall i\\leq t \\right)\n",
    "$$\n",
    "marginals can be found by summing over the non-marginalized variable\n",
    "$$\n",
    "p(s^\\prime \\, | \\, s,a) = \\sum_{r \\in R} p(s^\\prime,r \\, |\\, s,a), \\qquad\n",
    "p(r\\, | \\, s,a) = \\sum_{s^\\prime \\in S} p(s^\\prime,r \\, |\\, s,a)\n",
    "$$\n",
    "The markov decision process consists of 5 things:\n",
    "\n",
    "Set of states\n",
    "\n",
    "Set of actions\n",
    "\n",
    "Set of rewards\n",
    "\n",
    "State-transition probabilites and reward probabilities (defined jointly)\n",
    "\n",
    "Discount factor\n",
    "\n",
    "Written as 5-tuple (S,A,R,P,D)\n",
    "\n",
    "Decisons are made with a policy $\\pi$, which is the method by which agent navigates the environment\n",
    "\n",
    "The transitions are stochastic because the state is an imperfect representation of the environment\n",
    "\n",
    "Total future reward (which guides agent actions and does not include current reward) is represented by G:\n",
    "$$\n",
    "G(t) = \\sum_{\\tau=1}^\\infty d(\\tau)R (t+\\tau) \\textrm{ in these examples we have } d(\\tau)=\\gamma^{\\tau-1}\n",
    "$$\n",
    "\n",
    "Formal definition of value functions\n",
    "\n",
    "Value functions have a policy as a parameter, and state as an argument, and consider only future rewards (thus terminal states have value 0, unlike in tic-tac-toe example).\n",
    "$$\n",
    "V_\\pi(s):=E_\\pi \\left[ G(t) \\, | \\, S_t=s\\right]=E_\\pi \\left[ \\sum_{\\tau=0}^\\infty  R(t+\\tau+1) \\, | \\, S_t=s\\right]\n",
    "$$\n",
    "Note that this leads to a recursive formula when the discount factor is exponentially decaying:\n",
    "$$\n",
    "V_\\pi(s)=E_\\pi \\left[R(t+1)+ \\gamma G(t+1) \\, | \\, S_t=s\\right]= \\sum_{\\tau=0}^\\infty \\gamma^\\tau E_\\pi \\left[R(t+\\tau+1)\\, | \\, S_t=s\\right]\n",
    "$$\n",
    "Now the reward depends on the action and state, and the action depends on the policy $\\pi$, which we think of here as the probability of taking action a when in state s (given rewards etc.) $\\pi=\\pi(a|s)$\n",
    "Then the probability of getting reward r in state s is $$\n",
    "\\sum_a \\pi(a\\,|\\,s)p(r \\,\\vert\\, a,s),\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "E_\\pi \\left[R(t+1) \\, | \\, S_t=s\\right]=\\sum_r \\sum_a \\pi(a\\,|\\,s)r p(r \\,\\vert\\, a,s) =\n",
    "\\sum_a \\pi(a\\,|\\,s) \\sum_r \\sum_{s^\\prime}  r p( s^\\prime, \\,r \\,\\vert\\, a,s)\n",
    "$$\n",
    "Now we use the recursive form:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "V_\\pi(s) &=E_\\pi \\left[R(t+1)+ \\gamma \\sum_{\\tau=0}^\\infty  R(t+\\tau+2) \\, | \\, S_t=s\\right] \\\\\n",
    "&=\\sum_a \\pi(a\\,|\\,s) \\sum_r \\sum_{s^\\prime}p( s^\\prime, \\,r \\,\\vert\\, a,s)\\left(r+ \\gamma E_\\pi \\left[  \\sum_{\\tau=0}^\\infty  R(t+\\tau+2) \\, | \\, S_t=s^\\prime\\right] \\right) \\\\\n",
    "&= \\sum_a \\pi(a\\,|\\,s) \\sum_r \\sum_{s^\\prime}p( s^\\prime, \\,r \\,\\vert\\, a,s)\\left(r+ \\gamma V_\\pi(s^\\prime) \\right),\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "assuming that reward values do not change over time.  The final form above is the Bellman equation from dynamic programming.  The function $V_\\pi(s)$ is called the state value function, but we can also consider the action value function:\n",
    "$$\n",
    "Q_\\pi(s,a)=E_\\pi \\left[ G(t) \\, | \\, S_t=s,\\, A_t=a\n",
    "\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One policy is better than another iff the value of every state is greater than under teh other policy:\n",
    "$$\n",
    "\\pi_1 \\geq \\pi_2 \\iff V_{\\pi_1}(s)\\geq  V_{\\pi_2}(s) \\quad \\forall s \\in S\n",
    "$$\n",
    "A policy is optimal if none is better than it. For example if there are wo equal paths to a goal in gridworld.  The optimal state function would show the paths to be equal.  Optimal policies may not be unique, but the optimal value function, defined below, is.\n",
    "$$\n",
    "V_{*}(s) = \\max_\\pi \\{V_\\pi(s) \\} \\forall s\n",
    "$$\n",
    "Likewise, the optimal action-value function is defined as \n",
    "$$\n",
    "Q_{*}(s,a) = \\max_\\pi \\{Q_\\pi(s,a) \\} \\forall s, a\n",
    "$$\n",
    "The action-value function can be defined in terms of the state value function:\n",
    "$$\n",
    "Q_\\pi(s,a)=E \\left[R(t+1)+\\gamma V_\\pi(S_{t+1})\\, | \\, S_t=s,A_t=a  \\right]\n",
    "$$\n",
    "including for $\\pi = *$ the optimal policy.  Note that to get the optimal reward from a state, you must take the optimal action, so that \n",
    "$$\n",
    "V_{*}(s)=\\max_{a} \\{ Q_{*}(s,a) \\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue working with the Bellman equation:\n",
    "$$\n",
    " V_\\pi(s) =\\sum_a \\pi(a\\,|\\,s) \\sum_r \\sum_{s^\\prime}p( s^\\prime, \\,r \\,\\vert\\, a,s)\\left(r+ \\gamma V_\\pi(s^\\prime) \\right)\n",
    "$$\n",
    "Which for the optimal value function, must involve choosing the action of greatest value \n",
    "$$\n",
    "V_{*}(s) =\\max_a \\left\\{ \\sum_r \\sum_{s^\\prime}p( s^\\prime, \\,r \\,\\vert\\, a,s)\\left(r+ \\gamma V_{*}(s^\\prime) \\right) \\right\\}\n",
    "$$\n",
    "The last equation is the bellman optimality equation for the state value function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting together \n",
    "$$\n",
    "Q_{*}(s,a)=E \\left[R(t+1)+\\gamma V_{*}(S_{t+1})\\, | \\, S_t=s,A_t=a  \\right], \\qquad V_{*}(s)=\\max_{a} \\{ Q_{*}(s,a) \\},\n",
    "$$\n",
    "we obtain \n",
    "$$\n",
    "Q_{*}(s,a)=\\sum_{s^\\prime,r} p \\left(s^\\prime,\\, r \\, | \\, s,a \\right)\\left[ r+\\gamma \\max_{a^\\prime}Q_{*} (s^\\prime,a^\\prime)\\right]\n",
    "$$\n",
    "which says that the optimal reward for action a and state s can be found by averaging all the rewards and new states that can be reached and considering optimal rewards from those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Programming\n",
    "\n",
    "We find initialize the values of all states to be 0.  Then  we iteratively update the values for each state via:\n",
    "$$\n",
    "V_{k+1}=\\sum_a \\pi(a \\,|\\, s)\\sum_{s^\\prime,r}p(s^\\prime,r\\, |\\, s,a)\\{r+\\gamma V_k(s^\\prime) \\}\n",
    "$$\n",
    "Finding V is called the prediction problem, and finding the optimal policy is called the control problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The gridworld copied from\n",
    "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Grid: # Environment\n",
    "    def __init__(self, width, height, start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "\n",
    "    def set(self, rewards, actions):\n",
    "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "\n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "\n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "\n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "\n",
    "    def move(self, action):\n",
    "        # check if legal move first\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # return a reward (if any)\n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "\n",
    "    def undo_move(self, action):\n",
    "    # these are the opposite of what U/D/L/R should normally do\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # raise an exception if we arrive somewhere we shouldn't be\n",
    "        # should never happen\n",
    "        assert(self.current_state() in self.all_states())\n",
    "\n",
    "    def game_over(self):\n",
    "        # returns true if game is over, else false\n",
    "        # true if we are in a state where no actions are possible\n",
    "        return (self.i, self.j) not in self.actions\n",
    "\n",
    "    def all_states(self):\n",
    "        # possibly buggy but simple way to get all states\n",
    "        # either a position that has possible next actions\n",
    "        # or a position that yields a reward\n",
    "        return set(list(self.actions.keys()) + list(self.rewards.keys()))\n",
    "\n",
    "\n",
    "def standard_grid():\n",
    "    # define a grid that describes the reward for arriving at each state\n",
    "    # and possible actions at each state\n",
    "    # the grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .  .  .  1\n",
    "    # .  x  . -1\n",
    "    # s  .  .  .\n",
    "    g = Grid(3, 4, (2, 0))\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    actions = {\n",
    "        (0, 0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('L', 'D', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('L', 'R', 'U'),\n",
    "        (2, 3): ('L', 'U'),\n",
    "      }\n",
    "    g.set(rewards, actions)\n",
    "    return g\n",
    "\n",
    "\n",
    "def negative_grid(step_cost=-0.1):\n",
    "    # in this game we want to try to minimize the number of moves\n",
    "    # so we will penalize every move\n",
    "    g = standard_grid()\n",
    "    g.rewards.update({\n",
    "    (0, 0): step_cost,\n",
    "    (0, 1): step_cost,\n",
    "    (0, 2): step_cost,\n",
    "    (1, 0): step_cost,\n",
    "    (1, 2): step_cost,\n",
    "    (2, 0): step_cost,\n",
    "    (2, 1): step_cost,\n",
    "    (2, 2): step_cost,\n",
    "    (2, 3): step_cost,\n",
    "    })\n",
    "    return g\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative policy evaluation\n",
    "\n",
    "We start off with a uniformly random policy and deterministic state transition.  Thus \n",
    "$$\n",
    "\\pi(s\\, | \\, a) = \\frac{1}{|A(s)|}\n",
    "$$\n",
    "and $p(s^\\prime, r \\, | \\, s,a)$ is either 0 or 1.\n",
    "Second policy is if at start go to goal, otherwise go to losing state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "SMALL_ENOUGH=10**-4\n",
    "def print_values(V,g):\n",
    "    for i in range(g.width):\n",
    "        print(\"-------------------------\")\n",
    "        for j in range(g.height):\n",
    "            v=V.get((i,j),0)\n",
    "            if v>=0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "def print_policy(P,g):\n",
    "    for i in range(g.width):\n",
    "        print(\"\")\n",
    "        print(\"----------------\")\n",
    "        for j in range(g.height):\n",
    "            p=P.get((i,j),' ')\n",
    "            print(\" %s |\" % p,end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#here is where we do iterative policy evaluation\n",
    "grid=standard_grid()\n",
    "states=grid.all_states()\n",
    "V={}\n",
    "for s in states:\n",
    "    V[s]=0\n",
    "gamma=1.0 #discount factor for random policy\n",
    "count=0\n",
    "biggest_change=1.0\n",
    "\n",
    "while   biggest_change>SMALL_ENOUGH:\n",
    "    count+=1\n",
    "    biggest_change=0\n",
    "    change=[]\n",
    "    for s in states:\n",
    "        old_v=V[s]\n",
    "        \n",
    "        if s in grid.actions:\n",
    "            #bc only check if not terminal state\n",
    "            new_v=0\n",
    "            p_a=1.0 / len(grid.actions[s])#bc uniformly random\n",
    "            grid.set_state(s)\n",
    "            for a in grid.actions[s]:\n",
    "                \n",
    "                r=grid.move(a)\n",
    "                new_state=grid.current_state()\n",
    "                grid.undo_move(a)\n",
    "                new_v+=p_a * (r+gamma*V[new_state])\n",
    "            \n",
    "#                 print(\"Old v \"+ str(old_v)+\" new v \"+str(new_v)+\" diff \"+str(abs(old_v-new_v))+\" state\"+str(s))\n",
    "            V[s]=new_v\n",
    "            biggest_change = max(biggest_change,abs(old_v-new_v))\n",
    "#     print(\"max change is \"+str(max(change)))\n",
    "            \n",
    "#     print(biggest_change)\n",
    "#     print(biggest_change<SMALL_ENOUGH)\n",
    "print(\"Uniformly random actions\")\n",
    "print(str(count)+\"iterations to converge\")\n",
    "print_values(V,grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Fixed policy\")\n",
    "policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U',\n",
    "  }\n",
    "\n",
    "print_policy(policy, grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#redoing with fixed policy\n",
    "grid=standard_grid()\n",
    "states=grid.all_states()\n",
    "V={}\n",
    "for s in states:\n",
    "    V[s]=0\n",
    "gamma=.90 #discount factor for random policy\n",
    "count=0\n",
    "biggest_change=1.0\n",
    "while   biggest_change>SMALL_ENOUGH:\n",
    "    count+=1\n",
    "    biggest_change=0\n",
    "    change=[]\n",
    "    for s in states:\n",
    "        old_v=V[s]\n",
    "        \n",
    "        if s in policy:\n",
    "            #bc only check if not terminal state\n",
    "            new_v=0\n",
    "            p_a=1.0 #bc deterministic\n",
    "            grid.set_state(s)\n",
    "            a=policy[s]\n",
    "            r=grid.move(a)\n",
    "            new_state=grid.current_state()\n",
    "            grid.undo_move(a)\n",
    "            new_v+=p_a * (r+gamma*V[new_state]) #Averaging over the one possible state\n",
    "            V[s]=new_v\n",
    "            biggest_change = max(biggest_change,abs(old_v-new_v))\n",
    "#     print(\"max change is \"+str(max(change)))\n",
    "            \n",
    "#     print(biggest_change)\n",
    "#     print(biggest_change<SMALL_ENOUGH)\n",
    "print(str(count)+\"iterations to converge\")\n",
    "print_values(V,grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy improvement (finding better policies)\n",
    "\n",
    "Recall our equation $$\n",
    "V_\\pi(s) = Q_\\pi (s,\\pi(s))=\\sum_{s^\\prime,r}p(s^\\prime,r\\, | \\, s,\\pi(s))\\left( r+\\gamma V_\\pi (s^\\prime)\\right)\n",
    "$$\n",
    "We can change $pi(s)$ to a different action.  If we can search each action and each state, then we could just look to find $a \\in A$ such that $Q_\\pi(s,a)>Q_\\pi(s,\\pi(s)$ (note that $\\pi^\\prime$ is equal to $\\pi$ except at $s.$\n",
    "\n",
    "so we find a new policy with $V_\\pi^\\prime(s)\\geq V_\\pi(s)$.  If we have $Q$ then this ammounts to choosing\n",
    "$\\pi^\\prime(s) = \\text{argmax}_a Q_\\pi(s,a),$ and if we have $V$ it is $\\pi^\\prime(s) = $\n",
    "\n",
    "As we iterate through different policies, the value function will change.  We simply recalculate it given the new policy.  We can already find V given $\\pi$ (iterative policy evaluation).  We alternate between policy improvement and policy evaluation.  Keep it up until the policy does not change.  Once the policy becomes constant, so will the value function after one more policy evaluation iteration.\n",
    "\n",
    "Step 1:  randomly initialize $V(s),\\pi(s)$\n",
    "\n",
    "Step 2:  Find $V(S)$ given $pi$\n",
    "\n",
    "Step 3:  Update $\\pi$ go to 2 if it changed\n",
    "\n",
    "changed = F\n",
    "\n",
    "fo s in states:\n",
    "\n",
    "old_a = policy(s)\n",
    "\n",
    "policy(s) = best(a)\n",
    "\n",
    "if policy(s) != old_a then changed=T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "-------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "-------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "-------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "\n",
      "initial policy\n",
      "\n",
      "----------------\n",
      " L | D | R | 0 |\n",
      "----------------\n",
      " L | 0 | L | 0 |\n",
      "----------------\n",
      " D | L | L | L |\n",
      "3\n",
      "values\n",
      "-------------------------\n",
      " 0.71| 0.85| 1.00| 0.00|\n",
      "-------------------------\n",
      " 0.57| 0.00| 0.85| 0.00|\n",
      "-------------------------\n",
      " 0.44| 0.57| 0.71| 0.57|\n",
      "\n",
      "policy\n",
      "\n",
      "----------------\n",
      " R | R | R | 0 |\n",
      "----------------\n",
      " U | 0 | U | 0 |\n",
      "----------------\n",
      " U | R | U | L |"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "SMALL_ENOUGH = 10e-4\n",
    "GAMMA = 0.95\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "#world is deterministic so no transition probability\n",
    "grid=negative_grid()\n",
    "\n",
    "print(\"rewards\")\n",
    "print_values(grid.rewards,grid)\n",
    "print(\"\")\n",
    "#start initial policy as random\n",
    "policy={}\n",
    "for s in grid.actions.keys():\n",
    "    policy[s]=np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "print(\"initial policy\")\n",
    "print_policy(policy,grid)\n",
    "print(\"\")\n",
    "\n",
    "V={}\n",
    "states = grid.all_states()\n",
    "for s in states:\n",
    "    if s in grid.actions.keys():\n",
    "        V[s]=np.random.random()\n",
    "    else:\n",
    "        V[s]=0 #terminal state\n",
    "count =0\n",
    "while True and count<10: #start policy evaluation and improvement\n",
    "    count+=1\n",
    "    while True: #policy evaluation\n",
    "#         print(\"policy eval\")\n",
    "        biggest_change=0\n",
    "        for s in states:\n",
    "            old_v=V[s]\n",
    "        \n",
    "            if s in policy:\n",
    "                a=policy[s] #set current action\n",
    "                grid.set_state(s)\n",
    "                reward=grid.move(a)\n",
    "                V[s]=1*(reward+GAMMA*V[grid.current_state()])\n",
    "                biggest_change = max(biggest_change,np.abs(old_v - V[s]))\n",
    "        if biggest_change<SMALL_ENOUGH:\n",
    "            break\n",
    "    \n",
    "        #now policy improvement\n",
    "    policy_converged=True\n",
    "#     print(\"policy improvement\")\n",
    "    for s in states:\n",
    "        if s in policy:\n",
    "            old_a = policy[s]\n",
    "            new_a = None\n",
    "            best_value=float('-inf')\n",
    "            #loop through all possible actions to find the best current action\n",
    "            for a in ALL_POSSIBLE_ACTIONS:\n",
    "                grid.set_state(s)\n",
    "                reward=grid.move(a) #check reward from different action\n",
    "                value=1*(reward+GAMMA*V[grid.current_state()]) #get value\n",
    "                if value> best_value:\n",
    "                    best_value =value\n",
    "                    new_a=a\n",
    "            policy[s]=new_a #cannot be none because best value if -inf\n",
    "            if new_a!=old_a:\n",
    "                policy_converged=False\n",
    "\n",
    "    if policy_converged:\n",
    "        break\n",
    "        \n",
    "        \n",
    "print(count)\n",
    "print(\"values\")\n",
    "print_values(V,grid)\n",
    "print(\"\")\n",
    "print(\"policy\")\n",
    "print_policy(policy,grid)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "SMALL_ENOUGH = 10e-4\n",
    "GAMMA = 0.95\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "#world is deterministic so no transition probability\n",
    "grid=negative_grid(step_cost=-0.1)\n",
    "\n",
    "print(\"rewards\")\n",
    "print_values(grid.rewards,grid)\n",
    "print(\"\")\n",
    "#start initial policy as random\n",
    "policy={}\n",
    "for s in grid.actions.keys():\n",
    "    policy[s]=np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "print(\"initial policy\")\n",
    "print_policy(policy,grid)\n",
    "print(\"\")\n",
    "\n",
    "V={}\n",
    "states = grid.all_states()\n",
    "for s in states:\n",
    "    if s in grid.actions.keys():\n",
    "        V[s]=np.random.random()\n",
    "    else:\n",
    "        V[s]=0 #terminal state\n",
    "count =0\n",
    "while True and count<10: #start policy evaluation and improvement\n",
    "    count+=1\n",
    "    while True: #policy evaluation\n",
    "#         print(\"policy eval\")\n",
    "        biggest_change=0\n",
    "        for s in states:\n",
    "            old_v=V[s]\n",
    "            new_v=0\n",
    "            if s in policy:\n",
    "                for a in ALL_POSSIBLE_ACTIONS:\n",
    "                    if a==policy[s]:\n",
    "                        p=0.5\n",
    "                    else:\n",
    "                        p=0.5/3\n",
    "                \n",
    "                    grid.set_state(s)\n",
    "                    reward=grid.move(a)\n",
    "                    new_v+=p*(reward+GAMMA*V[grid.current_state()])\n",
    "                V[s]=new_v\n",
    "                biggest_change = max(biggest_change,np.abs(old_v - V[s]))\n",
    "        if biggest_change<SMALL_ENOUGH:\n",
    "            break\n",
    "    \n",
    "        #now policy improvement\n",
    "    policy_converged=True\n",
    "#     print(\"policy improvement\")\n",
    "    for s in states:\n",
    "        if s in policy:\n",
    "            old_a = policy[s]\n",
    "            new_a = None\n",
    "            best_value=float('-inf')\n",
    "            #loop through all possible actions to find the best current action\n",
    "            for a in ALL_POSSIBLE_ACTIONS: #chosen action\n",
    "                value=0\n",
    "                for a2 in ALL_POSSIBLE_ACTIONS:\n",
    "                    if a==a2:\n",
    "                        p=0.5\n",
    "                    else:\n",
    "                        p=0.5/3.0\n",
    "                    \n",
    "                    grid.set_state(s)\n",
    "                    reward=grid.move(a2) #check reward from different action\n",
    "                    value+=p*(reward+GAMMA*V[grid.current_state()]) #get value\n",
    "                if value> best_value:\n",
    "                    best_value =value\n",
    "                    new_a=a\n",
    "            policy[s]=new_a #cannot be none because best value if -inf\n",
    "            if new_a!=old_a:\n",
    "                policy_converged=False\n",
    "\n",
    "    if policy_converged:\n",
    "        break\n",
    "        \n",
    "        \n",
    "print(count)\n",
    "print(\"values\")\n",
    "print_values(V,grid)\n",
    "print(\"\")\n",
    "print(\"policy\")\n",
    "print_policy(policy,grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration is an alternative method for solving teh control problem (finding best policy).  Policy evaluation ends when V converges.  In practice, it can be the case that there is a point before convergence at which the greedy policy choice would not change, so instead of waiting for policy eval to finish we can just do a few steps because policy improvement would find the same policy anyway.\n",
    "\n",
    "Value iteration combines policy evaluation and improvement into one step:\n",
    "$$\n",
    "V_{k+1}(s)=\\max_a \\sum_{s^\\prime, r} p(s^\\prime, r\\, | \\, s,a)\\left( r+\\gamma V_k(s^\\prime)\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "-------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "-------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "-------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "\n",
      "initial policy\n",
      "\n",
      "----------------\n",
      " R | R | U |   |\n",
      "----------------\n",
      " D |   | U |   |\n",
      "----------------\n",
      " R | U | U | U |\n",
      "4\n",
      "values\n",
      "-------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "-------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "-------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "\n",
      "policy\n",
      "\n",
      "----------------\n",
      " R | R | R |   |\n",
      "----------------\n",
      " U |   | U |   |\n",
      "----------------\n",
      " U | R | U | L |"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "SMALL_ENOUGH = 10e-4\n",
    "GAMMA = .90\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "#world is deterministic so no transition probability\n",
    "grid=negative_grid()\n",
    "\n",
    "print(\"rewards\")\n",
    "print_values(grid.rewards,grid)\n",
    "print(\"\")\n",
    "#start initial policy as random\n",
    "policy={}\n",
    "for s in grid.actions.keys():\n",
    "    policy[s]=np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "print(\"initial policy\")\n",
    "print_policy(policy,grid)\n",
    "print(\"\")\n",
    "\n",
    "V={}\n",
    "states = grid.all_states()\n",
    "for s in states:\n",
    "    if s in grid.actions.keys():\n",
    "#         print(s)\n",
    "        V[s]=np.random.random()\n",
    "    else:\n",
    "        V[s]=0 #terminal state\n",
    "count =0\n",
    "while True: #start policy evaluation and improvement\n",
    "    count+=1\n",
    "    biggest_change=0\n",
    "    for s in states:\n",
    "        old_v=V[s]\n",
    "        if s in policy:\n",
    "            new_v=float('-inf')\n",
    "            for a in ALL_POSSIBLE_ACTIONS: #chosen action\n",
    "                grid.set_state(s)\n",
    "                r=grid.move(a) #check reward from different action\n",
    "                v=r+GAMMA*V[grid.current_state()] #get value\n",
    "               \n",
    "                if v> new_v:\n",
    "                    new_v =v\n",
    "            V[s]=new_v\n",
    "            biggest_change = max(biggest_change,np.abs(old_v - V[s]))\n",
    "    if biggest_change<SMALL_ENOUGH:\n",
    "        break\n",
    "\n",
    "#policy optimization done only once\n",
    "for s in policy.keys():\n",
    "    best_a = None\n",
    "    best_value=float('-inf')\n",
    "    for a in ALL_POSSIBLE_ACTIONS: #chosen action\n",
    "        grid.set_state(s)\n",
    "        r=grid.move(a) #check reward from different action\n",
    "        v=(r+GAMMA*V[grid.current_state()]) #get value\n",
    "        if v> best_value:\n",
    "            best_value =v\n",
    "            best_a=a\n",
    "    policy[s]=best_a \n",
    "        \n",
    "        \n",
    "print(count)\n",
    "print(\"values\")\n",
    "print_values(V,grid)\n",
    "print(\"\")\n",
    "print(\"policy\")\n",
    "print_policy(policy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
