{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD learning combines ideas from dynamic programming and MC.  DP requires full model and never learns from experience.  MC does not require this and can lear, but can only update after completing an episode, while DP uses bootstrapping (iterative estimation).  TD is fully online and can update during an episode, which is important for continuous tasks with no episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First prediction and then control.  There are two control methods:  SARSA and Q learning.  We will apply TD(0), specific case of $TD(\\lambda)$.\n",
    "The general method for finding averages is to using a moving average with exponential decay $$\n",
    "V_{k+1}(S_t)=V_k(S_t)+\\alpha \\left(G(t)-V_k(S_t) \\right)\n",
    "$$  Further, \n",
    "$$\n",
    "V(s):=E \\left[G(t) | S_t =s \\right]=E \\left[R(t+1)+\\gamma V(S_{t+1}) | S_t =s \\right]\n",
    "$$\n",
    "If we use the moving average with the recursive definition we obtain:\n",
    "$$\n",
    "V_{k+1}(s) = V_k(s)+ \\alpha \\left[r+\\gamma V_k(s^\\prime) -V_k(s) \\right]\n",
    "$$\n",
    "This is online because we can update after we determine the state $s^\\prime$ (if we know $G(t)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MC, randomness arises because an episode can play out in different ways (stochastic policy or tranistions).  in TD(0) we are estimating from other estimates ($r+\\gamma V(s)$ is an estimate of G) (bootstrapping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Grid: # Environment\n",
    "    def __init__(self, width, height, start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "\n",
    "    def set(self, rewards, actions):\n",
    "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "\n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "\n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "\n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "\n",
    "    def move(self, action):\n",
    "        # check if legal move first\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # return a reward (if any)\n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "\n",
    "    def undo_move(self, action):\n",
    "    # these are the opposite of what U/D/L/R should normally do\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # raise an exception if we arrive somewhere we shouldn't be\n",
    "        # should never happen\n",
    "        assert(self.current_state() in self.all_states())\n",
    "\n",
    "    def game_over(self):\n",
    "        # returns true if game is over, else false\n",
    "        # true if we are in a state where no actions are possible\n",
    "        return (self.i, self.j) not in self.actions\n",
    "\n",
    "    def all_states(self):\n",
    "        # possibly buggy but simple way to get all states\n",
    "        # either a position that has possible next actions\n",
    "        # or a position that yields a reward\n",
    "        return set(list(self.actions.keys()) + list(self.rewards.keys()))\n",
    "\n",
    "\n",
    "def standard_grid():\n",
    "    # define a grid that describes the reward for arriving at each state\n",
    "    # and possible actions at each state\n",
    "    # the grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .  .  .  1\n",
    "    # .  x  . -1\n",
    "    # s  .  .  .\n",
    "    g = Grid(3, 4, (2, 0))\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    actions = {\n",
    "        (0, 0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('L', 'D', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('L', 'R', 'U'),\n",
    "        (2, 3): ('L', 'U'),\n",
    "      }\n",
    "    g.set(rewards, actions)\n",
    "    return g\n",
    "\n",
    "\n",
    "def negative_grid(step_cost=-0.1):\n",
    "    # in this game we want to try to minimize the number of moves\n",
    "    # so we will penalize every move\n",
    "    g = standard_grid()\n",
    "    g.rewards.update({\n",
    "    (0, 0): step_cost,\n",
    "    (0, 1): step_cost,\n",
    "    (0, 2): step_cost,\n",
    "    (1, 0): step_cost,\n",
    "    (1, 2): step_cost,\n",
    "    (2, 0): step_cost,\n",
    "    (2, 1): step_cost,\n",
    "    (2, 2): step_cost,\n",
    "    (2, 3): step_cost,\n",
    "    })\n",
    "    return g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SMALL_ENOUGH=10**-4\n",
    "def print_values(V,g):\n",
    "    for i in range(g.width):\n",
    "        print(\"-------------------------\")\n",
    "        for j in range(g.height):\n",
    "            v=V.get((i,j),0)\n",
    "            if v>=0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "def print_policy(P,g):\n",
    "    for i in range(g.width):\n",
    "        print(\"\")\n",
    "        print(\"----------------\")\n",
    "        for j in range(g.height):\n",
    "            p=P.get((i,j),' ')\n",
    "            print(\" %s |\" % p,end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_POSSIBLE_ACTIONS=('U','D','L','R')\n",
    "GAMMA=0.9\n",
    "ALPHA=0.1\n",
    "SMALL_ENOUGH =10e-4\n",
    "def random_action(a,eps=.1):\n",
    "    p=np.random.random()\n",
    "    if p<(1-eps):\n",
    "        return a\n",
    "    else:\n",
    "        return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "#\n",
    "def play_game(grid,policy):\n",
    "    s=(2,0)\n",
    "    grid.set_state(s)\n",
    "    state_reward=[(s,0)]\n",
    "    while not grid.game_over():\n",
    "        a=policy[s]\n",
    "        a=random_action(a)\n",
    "        r=grid.move(a)\n",
    "        s=grid.current_state()\n",
    "        state_reward.append((s,r))\n",
    "    return state_reward\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stardard_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-882c24fd75fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstardard_grid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# print rewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rewards:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stardard_grid' is not defined"
     ]
    }
   ],
   "source": [
    "grid=stardard_grid()\n",
    "# print rewards\n",
    "print(\"rewards:\")\n",
    "print_values(grid.rewards, grid)\n",
    "\n",
    "# state -> action\n",
    "policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'U',\n",
    "    (2, 1): 'L',\n",
    "    (2, 2): 'U',\n",
    "    (2, 3): 'L',\n",
    "  }\n",
    "\n",
    "V = {}\n",
    "states = grid.all_states()\n",
    "for s in states:\n",
    "    V[s] = 0\n",
    "    \n",
    "#now that everything is initialized we start the process of going through states\n",
    "for t in range(5000):\n",
    "    #generate episode\n",
    "    state_reward=play_game(grid,policy)\n",
    "    #ignore first one\n",
    "    for t in range(len(state_reward)-1):\n",
    "        s,_ = state_reward[t]\n",
    "        s2,r=state_reward[t+1]\n",
    "        V[s]=V[s]+ALPHA*(r+GAMMA*V[s2]-V[s])\n",
    "print(\"values\")\n",
    "print(\"\")\n",
    "print_values(V,grid)\n",
    "print(\"policy\")\n",
    "print_policy(policy,grid)\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
