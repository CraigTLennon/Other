{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of generalized policy iteration involved doing policy eval and then greedy improvement.  These methods, which use current best policy, are on-policy methods.  Q learning is an off-policy method.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q learning looks similar to sarsa in the update step:\n",
    "\n",
    "SARSA\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma Q(s^\\prime,a^\\prime)-Q(s,a) \\right]\n",
    "$$\n",
    "\n",
    "Q-learn\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a^\\prime}Q(s^\\prime,a^\\prime)-Q(s,a) \\right],\n",
    "$$\n",
    "where we are updating Q based on the max over all actions.\n",
    "It seems like these chould be the same, since isn't $a^\\prime = \\text{argmax}_{a^\\prime} \\left\\{ Q(s^\\prime,a) \\right\\}$.  This part is true, but the difference is that because Q learning is off policy, we do not actually need to do $a^\\prime$ next.  We use $Q(s^\\prime,a^\\prime)$ even if we do not do $a^\\prime$ next.  It does not matter what policy we follow, we can choose random actions, although it may take a long time for the episode to finish.  If, however, we use a greedy policy for Q learning, then we are doing Q-learning and sarsa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
