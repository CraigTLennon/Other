{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to apply TD(0) to control.  We alternate between using TD(0) for prediction, and then doing policy improvement by greedy action selection.  We update Q in place, improving the policy after each change like we did in MC.  Q has the same recursive form as in MC, and the same limitations (e.g. needing lots of samples).  As in MC, the update equation is \n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma Q(s^\\prime,a^\\prime)-Q(s,a) \\right]\n",
    "$$\n",
    "which requires the tuple $(s,a,r,s^\\prime,a^\\prime).$  We use eps greedy to find new states.  Convergence for sarsa has not been published, but the belief is that it will converge if policy converges to greedy policy.  can set $\\epsilon =c/t^a$ to make it less exploratory over time.  The learning rate $\\alpha$ can also decay.  We don't want to use a function of t because  not eavery state is updated at a time step, and we don't want to weaken learning for states we have never seen, even in a given episode.  We change based on the number of $(s,a)$ pairs we have seen:\n",
    "$$\n",
    "\\alpha(s,a)=\\frac{\\alpha_0}{\\text{count}(s,a)} \\quad \\text{or} \\quad \\alpha(s,a)=\\frac{\\alpha_0}{k+m*\\text{count}(s,a)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Grid: # Environment\n",
    "    def __init__(self, width, height, start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "\n",
    "    def set(self, rewards, actions):\n",
    "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "\n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "\n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "\n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "\n",
    "    def move(self, action):\n",
    "        # check if legal move first\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # return a reward (if any)\n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "\n",
    "    def undo_move(self, action):\n",
    "    # these are the opposite of what U/D/L/R should normally do\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # raise an exception if we arrive somewhere we shouldn't be\n",
    "        # should never happen\n",
    "        assert(self.current_state() in self.all_states())\n",
    "\n",
    "    def game_over(self):\n",
    "        # returns true if game is over, else false\n",
    "        # true if we are in a state where no actions are possible\n",
    "        return (self.i, self.j) not in self.actions\n",
    "\n",
    "    def all_states(self):\n",
    "        # possibly buggy but simple way to get all states\n",
    "        # either a position that has possible next actions\n",
    "        # or a position that yields a reward\n",
    "        return set(list(self.actions.keys()) + list(self.rewards.keys()))\n",
    "\n",
    "\n",
    "def standard_grid():\n",
    "    # define a grid that describes the reward for arriving at each state\n",
    "    # and possible actions at each state\n",
    "    # the grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .  .  .  1\n",
    "    # .  x  . -1\n",
    "    # s  .  .  .\n",
    "    g = Grid(3, 4, (2, 0))\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    actions = {\n",
    "        (0, 0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('L', 'D', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('L', 'R', 'U'),\n",
    "        (2, 3): ('L', 'U'),\n",
    "      }\n",
    "    g.set(rewards, actions)\n",
    "    return g\n",
    "\n",
    "\n",
    "def negative_grid(step_cost=-0.1):\n",
    "    # in this game we want to try to minimize the number of moves\n",
    "    # so we will penalize every move\n",
    "    g = standard_grid()\n",
    "    g.rewards.update({\n",
    "    (0, 0): step_cost,\n",
    "    (0, 1): step_cost,\n",
    "    (0, 2): step_cost,\n",
    "    (1, 0): step_cost,\n",
    "    (1, 2): step_cost,\n",
    "    (2, 0): step_cost,\n",
    "    (2, 1): step_cost,\n",
    "    (2, 2): step_cost,\n",
    "    (2, 3): step_cost,\n",
    "    })\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def print_values(V,g):\n",
    "    for i in range(g.width):\n",
    "        print(\"-------------------------\")\n",
    "        for j in range(g.height):\n",
    "            v=V.get((i,j),0)\n",
    "            if v>=0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "def print_policy(P,g):\n",
    "    for i in range(g.width):\n",
    "        print(\"\")\n",
    "        print(\"----------------\")\n",
    "        for j in range(g.height):\n",
    "            p=P.get((i,j),' ')\n",
    "            print(\" %s |\" % p,end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_dict(d): #just a helper function for getting max from dict\n",
    "    max_key=None\n",
    "    max_val = float('-inf')\n",
    "    for k,v in d.items():\n",
    "        if v>max_val:\n",
    "            max_val=v\n",
    "            max_key=k\n",
    "    return max_key,max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SMALL_ENOUGH = 10e-4\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# NOTE: this is only policy evaluation, not optimization\n",
    "\n",
    "def random_action(a, eps=0.1):\n",
    "  # we'll use epsilon-soft to ensure all states are visited\n",
    "  # what happens if you don't do this? i.e. eps=0\n",
    "  p = np.random.random()\n",
    "  if p < (1 - eps):\n",
    "    return a\n",
    "  else:\n",
    "    return np.random.choice(ALL_POSSIBLE_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "-------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "-------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "-------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "iter 0\n",
      "iter 2000\n",
      "iter 4000\n",
      "iter 6000\n",
      "iter 8000\n",
      "iter 10000\n",
      "iter 12000\n",
      "iter 14000\n",
      "iter 16000\n",
      "iter 18000\n",
      "iter 20000\n",
      "iter 22000\n",
      "iter 24000\n",
      "iter 26000\n",
      "iter 28000\n",
      "iter 30000\n",
      "iter 32000\n",
      "iter 34000\n",
      "iter 36000\n",
      "iter 38000\n",
      "iter 40000\n",
      "iter 42000\n",
      "iter 44000\n",
      "iter 46000\n",
      "iter 48000\n",
      "iter 50000\n",
      "iter 52000\n",
      "iter 54000\n",
      "iter 56000\n",
      "iter 58000\n",
      "iter 60000\n",
      "iter 62000\n",
      "iter 64000\n",
      "iter 66000\n",
      "iter 68000\n",
      "iter 70000\n",
      "iter 72000\n",
      "iter 74000\n",
      "iter 76000\n",
      "iter 78000\n",
      "iter 80000\n",
      "iter 82000\n",
      "iter 84000\n",
      "iter 86000\n",
      "iter 88000\n",
      "iter 90000\n",
      "iter 92000\n",
      "iter 94000\n",
      "iter 96000\n",
      "iter 98000\n",
      "update counts\n",
      "-------------------------\n",
      " 0.19| 0.18| 0.18| 0.00|\n",
      "-------------------------\n",
      " 0.19| 0.00| 0.03| 0.00|\n",
      "-------------------------\n",
      " 0.19| 0.02| 0.02| 0.00|\n",
      "values\n",
      "\n",
      "-------------------------\n",
      " 0.58| 0.78| 1.00| 0.00|\n",
      "-------------------------\n",
      " 0.41| 0.00| 0.78| 0.00|\n",
      "-------------------------\n",
      " 0.25| 0.32| 0.54| 0.20|\n",
      "policy\n",
      "\n",
      "----------------\n",
      " R | R | R |   |\n",
      "----------------\n",
      " U |   | U |   |\n",
      "----------------\n",
      " U | R | U | L |"
     ]
    }
   ],
   "source": [
    "grid=negative_grid()\n",
    "print(\"rewards:\")\n",
    "print_values(grid.rewards, grid)\n",
    "\n",
    "Q={}\n",
    "returns={}\n",
    "states = grid.all_states()\n",
    "#this is just initialization\n",
    "for s in states:\n",
    "        Q[s]={}\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "            Q[s][a]=0\n",
    "    \n",
    "update_ct={} #see what proportion of time we spend in each state\n",
    "#would be interesting to see what states are \"attractive\"\n",
    "update_ct_sa={} #for the adaptive learning rate\n",
    "for s in states:\n",
    "    update_ct_sa[s]={}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "        update_ct_sa[s][a]=1.0\n",
    "        \n",
    "#until convergence (but limit here to n runs)\n",
    "t=1.0\n",
    "deltas=[]#for debug\n",
    "for it in range(100000):\n",
    "    if it % 100 ==0:\n",
    "        t+=10e-3\n",
    "    if it %2000==0:\n",
    "        print(\"iter \"+str(it))\n",
    "\n",
    "    #main loop\n",
    "    s=(2,0)\n",
    "    grid.set_state(s)\n",
    "    a=max_dict(Q[s])[0] #choose best action\n",
    "    a=random_action(a,eps=0.95/t)\n",
    "    biggest_change=0\n",
    "    while not grid.game_over():\n",
    "        r=grid.move(a)\n",
    "        s2=grid.current_state()\n",
    "        a2=max_dict(Q[s2])[0] \n",
    "        a2=random_action(a2,eps=0.95/t)\n",
    "        \n",
    "        #update Q as we go\n",
    "        alp=ALPHA/update_ct_sa[s][a]\n",
    "        update_ct_sa[s][a]+=.005\n",
    "        old_q=Q[s][a]\n",
    "        Q[s][a]=Q[s][a]+alp*(r+GAMMA*Q[s2][a2]-Q[s][a])\n",
    "        biggest_change=max(biggest_change,abs(old_q-Q[s][a]))\n",
    "        update_ct[s]=update_ct.get(s,0)+1\n",
    "        \n",
    "        #now next state becoems current\n",
    "        s=s2\n",
    "        a=a2\n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "policy={}\n",
    "V={}\n",
    "for s in grid.actions.keys():\n",
    "    a,max_q=max_dict(Q[s])\n",
    "    policy[s]=a\n",
    "    V[s]=max_q\n",
    "    \n",
    "print(\"update counts\")\n",
    "total = np.sum(list(update_ct.values()))\n",
    "for k,v in update_ct.items():\n",
    "    update_ct[k]=float(v)/total\n",
    "print_values(update_ct,grid)\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"values\")\n",
    "print(\"\")\n",
    "print_values(V,grid)\n",
    "print(\"policy\")\n",
    "print_policy(policy,grid)\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
