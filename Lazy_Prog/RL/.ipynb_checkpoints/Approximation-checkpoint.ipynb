{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods used to estimate V and Q require $|S|$ or $|S| \\times |A|$ values, and approximation can be used for estimates with fewer values.  We do feature extraction to convert states s to features x ($x=\\phi(s)$), then approximate V with a function with parameter theta ($f(x,\\theta)\\approx V(s)$).  These methods require differentiable models (linear regression and gradient descent).  Start with linear function approximation applied to MC prediction.  Feature engineering is important for this type of thing.\n",
    "\n",
    "We will be using sqaured error as our cost function.\n",
    "\n",
    "We replace the expected value of V with the sample mean in the error function:\n",
    "$$\n",
    "\\text{Error} = \\left[ \\frac{1}{N} \\sum_{i=1}^N G_{i,s} - \\hat V(s) \\right]^2\n",
    "$$\n",
    "With this error function, we can use stochastic gradient descent.  V has $\\theta$ as a parameter, and in gradient descent we make use of the fact that the negative of the gradient points in the direction of steepest descent.  So to go to a local min starting at a:\n",
    "$$\n",
    "a_{k+1}=a_k-\\gamma \\nabla f(a_k)\n",
    "$$\n",
    "\n",
    "So to miniize error, we change theta by taking steps to minimize the gradient at rate alpha \n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha \\frac{\\partial E}{\\partial \\theta}\n",
    "$$\n",
    "In general, \n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\left( G- \\hat V \\right) \\frac{\\partial E}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "For linear models, \n",
    "$$\n",
    "\\hat V(s,\\theta) = \\theta^T \\phi (s)= \\theta^T x\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\hat V}{\\partial \\theta}  = x\n",
    "$$\n",
    "thus\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\left( G- \\hat V \\right) x\n",
    "$$\n",
    "Back in the MC case, $V(s)$ was itself the parameter (with grad 1) leading to the update equation we actually used\n",
    "$$\n",
    "V(s) \\leftarrow  V(s) + \\alpha \\left( G_s - V(s) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider finding the features.  We can treat states as categorical vectors and use one hot encoding.  Then the dimension of the resulting vector would be the number of states.  The problem is this uses the same number of parameters as the old way $|S|,$ therefore we do not use it except for debugging and testing.\n",
    "\n",
    "For our gridworld example, we are going to let (i,j) be the 2 dimensional vector in x-y space, potentially scaled to mean 0 and variance 1.  However, we will use a polynomial model to allow for more expression.\n",
    "\n",
    "We will start with a fixed policy (the prediction problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Grid: # Environment\n",
    "    def __init__(self, width, height, start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "\n",
    "    def set(self, rewards, actions):\n",
    "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "\n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "\n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "\n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "\n",
    "    def move(self, action):\n",
    "        # check if legal move first\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # return a reward (if any)\n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "\n",
    "    def undo_move(self, action):\n",
    "    # these are the opposite of what U/D/L/R should normally do\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # raise an exception if we arrive somewhere we shouldn't be\n",
    "        # should never happen\n",
    "        assert(self.current_state() in self.all_states())\n",
    "\n",
    "    def game_over(self):\n",
    "        # returns true if game is over, else false\n",
    "        # true if we are in a state where no actions are possible\n",
    "        return (self.i, self.j) not in self.actions\n",
    "\n",
    "    def all_states(self):\n",
    "        # possibly buggy but simple way to get all states\n",
    "        # either a position that has possible next actions\n",
    "        # or a position that yields a reward\n",
    "        return set(list(self.actions.keys()) + list(self.rewards.keys()))\n",
    "\n",
    "\n",
    "def standard_grid():\n",
    "    # define a grid that describes the reward for arriving at each state\n",
    "    # and possible actions at each state\n",
    "    # the grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .  .  .  1\n",
    "    # .  x  . -1\n",
    "    # s  .  .  .\n",
    "    g = Grid(3, 4, (2, 0))\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    actions = {\n",
    "        (0, 0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('L', 'D', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('L', 'R', 'U'),\n",
    "        (2, 3): ('L', 'U'),\n",
    "      }\n",
    "    g.set(rewards, actions)\n",
    "    return g\n",
    "\n",
    "\n",
    "def negative_grid(step_cost=-0.1):\n",
    "    # in this game we want to try to minimize the number of moves\n",
    "    # so we will penalize every move\n",
    "    g = standard_grid()\n",
    "    g.rewards.update({\n",
    "    (0, 0): step_cost,\n",
    "    (0, 1): step_cost,\n",
    "    (0, 2): step_cost,\n",
    "    (1, 0): step_cost,\n",
    "    (1, 2): step_cost,\n",
    "    (2, 0): step_cost,\n",
    "    (2, 1): step_cost,\n",
    "    (2, 2): step_cost,\n",
    "    (2, 3): step_cost,\n",
    "    })\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SMALL_ENOUGH=10**-4\n",
    "def print_values(V,g):\n",
    "    for i in range(g.width):\n",
    "        print(\"-------------------------\")\n",
    "        for j in range(g.height):\n",
    "            v=V.get((i,j),0)\n",
    "            if v>=0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "def print_policy(P,g):\n",
    "    for i in range(g.width):\n",
    "        print(\"\")\n",
    "        print(\"----------------\")\n",
    "        for j in range(g.height):\n",
    "            p=P.get((i,j),' ')\n",
    "            print(\" %s |\" % p,end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_action(a):\n",
    "    p=np.random.random()\n",
    "    if p<.5:\n",
    "        return a\n",
    "    else:\n",
    "        tmp=list(ALL_POSSIBLE_ACTIONS)\n",
    "        tmp.remove(a)\n",
    "        return np.random.choice(tmp)\n",
    "    \n",
    "\n",
    "ALL_POSSIBLE_ACTIONS=('U','D','L','R')\n",
    "GAMMA=0.9\n",
    "def play_game(grid,policy):\n",
    "    #return states and returns\n",
    "    #reset to start at random posn,\n",
    "    \n",
    "    start_states=list(grid.actions.keys())\n",
    "    start_idx=np.random.choice(len(start_states))\n",
    "    grid.set_state(start_states[start_idx])\n",
    "    \n",
    "    s=grid.current_state()\n",
    "    state_reward=[(s,0)] #state reward tuple\n",
    "    while not grid.game_over():\n",
    "        a=policy[s]\n",
    "        a=random_action(a) \n",
    "        r=grid.move(a)\n",
    "        s=grid.current_state()\n",
    "        state_reward.append((s,r))\n",
    "    G=0\n",
    "    state_return=[]\n",
    "    first = True\n",
    "#     print(state_reward)\n",
    "    for s,r in reversed(state_reward):\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            #ignore first state bc value for terminal state is 0\n",
    "            state_return.append((s,G))\n",
    "        G=r+GAMMA*G\n",
    "#     print(state_return)\n",
    "    state_return.reverse()\n",
    "    \n",
    "    return state_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "-------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "-------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "-------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "values\n",
      "\n",
      "-------------------------\n",
      " 0.44| 0.54| 0.64| 0.00|\n",
      "-------------------------\n",
      " 0.34| 0.00| 0.29| 0.00|\n",
      "-------------------------\n",
      " 0.24| 0.09|-0.06|-0.21|\n",
      "policy\n",
      "\n",
      "----------------\n",
      " R | R | R |   |\n",
      "----------------\n",
      " U |   | U |   |\n",
      "----------------\n",
      " U | L | U | L |"
     ]
    }
   ],
   "source": [
    "grid = standard_grid()\n",
    "\n",
    "# print rewards\n",
    "print(\"rewards:\")\n",
    "print_values(grid.rewards, grid)\n",
    "LEARNING_RATE=0.001\n",
    "\n",
    "\n",
    "# state -> action\n",
    "policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'U',\n",
    "    (2, 1): 'L',\n",
    "    (2, 2): 'U',\n",
    "    (2, 3): 'L',\n",
    "  }\n",
    "\n",
    "#initialize theta for our model V=theta.dot(x) with\n",
    "#x=[row,col,row*col,1]-1 for bias term\n",
    "theta=np.random.randn(4)/2\n",
    "def s2x(s):\n",
    "    return np.array([s[0]-1,s[1]-1,s[0]*s[1]-3,1])#why minus 3?\n",
    "\n",
    "#repeat until converge\n",
    "deltas=[]\n",
    "t=1.0\n",
    "for it in range(20000):\n",
    "    if it % 100 ==0:\n",
    "        t+=0.01\n",
    "    alpha=LEARNING_RATE\n",
    "    biggest_change=0\n",
    "    state_return=play_game(grid,policy)\n",
    "    seen_states=set()\n",
    "    for s,G in state_return:\n",
    "        if s not in seen_states:\n",
    "            old_theta=theta.copy()\n",
    "            x=s2x(s)\n",
    "            V_hat=theta.dot(x)\n",
    "            theta+=alpha*(G-V_hat)*x\n",
    "            biggest_change=max(biggest_change,np.abs(old_theta-theta).sum())\n",
    "            seen_states.add(s)\n",
    "        deltas.append(biggest_change)\n",
    "#now get the values to print out\n",
    "V = {}\n",
    "states = grid.all_states()\n",
    "for s in states:\n",
    "    if s in grid.actions.keys():\n",
    "        V[s]=theta.dot(s2x(s))\n",
    "    else:\n",
    "  # terminal state or state we can't otherwise get to\n",
    "        V[s] = 0\n",
    "            \n",
    "print(\"values\")\n",
    "print(\"\")\n",
    "print_values(V,grid)\n",
    "print(\"policy\")\n",
    "print_policy(policy,grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximation for TD(0)\n",
    "\n",
    "In TD we update before the end, using the reward and value for the next state instead of G.  So we are using the model output as a target to fix model parameters.  This is a semi-gradient method, because the target we are using is not a true target, and thus the gradient is not a true gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here we need the play game function from TD0\n",
    "def random_action(a, eps=0.1):\n",
    "    # we'll use epsilon-soft to ensure all states are visited\n",
    "    # what happens if you don't do this? i.e. eps=0\n",
    "    p = np.random.random()\n",
    "    if p < (1 - eps):\n",
    "        return a\n",
    "    else:\n",
    "        return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "def play_game_td(grid, policy):\n",
    "    # returns a list of states and corresponding rewards (not returns as in MC)\n",
    "    # start at the designated start state\n",
    "    s = (2, 0)\n",
    "    grid.set_state(s)\n",
    "    states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
    "    while not grid.game_over():\n",
    "        a = policy[s]\n",
    "        a = random_action(a)\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        states_and_rewards.append((s, r))\n",
    "    return states_and_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.theta=np.random.randn(4)/2\n",
    "    def s2x(self,s):\n",
    "        return np.array([s[0]-1,s[1]-1,s[0]*s[1]-3,1])\n",
    "    def predict(self,x):\n",
    "        x=self.s2x(s)\n",
    "        return self.theta.dot(x)\n",
    "    def grad(self,s):\n",
    "        return self.s2x(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "-------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "-------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "-------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "values\n",
      "\n",
      "-------------------------\n",
      " 0.44| 0.54| 0.64| 0.00|\n",
      "-------------------------\n",
      " 0.34| 0.00| 0.29| 0.00|\n",
      "-------------------------\n",
      " 0.24| 0.09|-0.06|-0.21|\n",
      "policy\n",
      "\n",
      "----------------\n",
      " R | R | R |   |\n",
      "----------------\n",
      " U |   | R |   |\n",
      "----------------\n",
      " U | R | R | U |"
     ]
    }
   ],
   "source": [
    "grid = standard_grid()\n",
    "ALPHA=.001\n",
    "# print rewards\n",
    "print(\"rewards:\")\n",
    "print_values(grid.rewards, grid)\n",
    "# state -> action\n",
    "policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U',\n",
    "  }\n",
    "model=Model()\n",
    "deltas=[]\n",
    "k=1.0\n",
    "for it in range(20000):\n",
    "    if it % 10 ==0:\n",
    "        k+=0.01\n",
    "    alpha=ALPHA/k\n",
    "    biggest_change=0\n",
    "    \n",
    "    state_reward = play_game_td(grid,policy)\n",
    "    for t in range(len(state_reward)-1):\n",
    "        s,_ = state_reward[t]\n",
    "        s2,r=state_reward[t+1]\n",
    "        old_theta = model.theta.copy()\n",
    "        if grid.is_terminal(s2):\n",
    "            target=r\n",
    "        else: #here is where we use an estimate as target\n",
    "            target = r+GAMMA*model.predict(s2)\n",
    "        model.theta+=alpha*(target-model.predict(s))*model.grad(s)\n",
    "        biggest_change=max(biggest_change,np.abs(old_theta-model.theta).sum())\n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "V = {}\n",
    "states = grid.all_states()\n",
    "for s in states:\n",
    "    if s in grid.actions.keys():\n",
    "        V[s]=theta.dot(s2x(s))\n",
    "    else:\n",
    "  # terminal state or state we can't otherwise get to\n",
    "        V[s] = 0\n",
    "            \n",
    "print(\"values\")\n",
    "print(\"\")\n",
    "print_values(V,grid)\n",
    "print(\"policy\")\n",
    "print_policy(policy,grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
