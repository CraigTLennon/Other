{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The markov property\n",
    "A sequence on random variables has the markov property if the probability of the next value (state) depends only on the current state, and not on previous states given knowledge of the current state.  \n",
    "$$\n",
    "P( x_{i+1}\\, \\vert \\, x_i)=P( x_{i+1}\\, \\vert \\, x_j, \\forall \\, j\\le i )\n",
    "$$\n",
    "When there are a finite number of states, the transisiton probabilities can be represented by a transition martix $A(i,j)=P( x_{t}=j\\, \\vert \\, x_{t-1}=i)$.  With large state spaces, it may not be possible to get training data for every state transition.  Rather than setting the transition probabilities to zero, smoothing is used.  Without smoothing\n",
    "$$\n",
    "A(i,j)= \\frac{count(i \\rightarrow j)}{count(i)}\n",
    "$$\n",
    "With smoothing, we use $$\n",
    "A(i,j)= \\frac{count(i \\rightarrow j) +\\epsilon}{count(i)+\\epsilon N}\n",
    "$$\n",
    "Where N is the number of states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider a second order model for generating phrases trained on Robert Frost poetry.  In this model, we train to predict a word given the previous two words $$\n",
    "P(w_{t+1} \\, \\vert \\, w_t, w_{t-1})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "#for removing the punctuation from the poem\n",
    "def remove_pn(s):\n",
    "    translator=str.maketrans( '','',string.punctuation)\n",
    "    return([st.translate(translator) for st in s ])\n",
    "\n",
    "def add2dict(d,k,v):\n",
    "    if k not in d:\n",
    "        d[k]=[]\n",
    "    d[k].append(v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'l', 'l', '', ' ', 't', 'a', 'l', 'k']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tests\n",
    "s=\"all, talk\"\n",
    "remove_pn(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionaries for the first, second, and other words in sentences.\n",
    "init={}\n",
    "second={}\n",
    "transitions={}\n",
    "for line in open('robert_frost.txt'):\n",
    "    tokens=remove_pn(line.rstrip().lower().split())\n",
    "    N=len(tokens)\n",
    "    for i in range(N):\n",
    "        t=tokens[i]\n",
    "        if i==0:\n",
    "            init[t]=init.get(t,0)+1\n",
    "        else:\n",
    "            t1=tokens[i-1]\n",
    "            if i==N-1:\n",
    "                add2dict(transitions, (t1,t),'END')\n",
    "            if i ==1:\n",
    "                add2dict(second, t1,t)\n",
    "            else:\n",
    "                t2=tokens[i-2]\n",
    "                add2dict(transitions, (t2,t1),t)\n",
    "                \n",
    "#normlize distributions\n",
    "init_total=sum(init.values())\n",
    "for t,c in init.items():\n",
    "    init[t]=c/init_total\n",
    "    \n",
    "def list2prob(ts):\n",
    "    d={}\n",
    "    n=len(ts)\n",
    "    for t in ts:\n",
    "        d[t]=d.get(t,0)+1\n",
    "    for t,c, in d.items():\n",
    "        d[t]=c/n\n",
    "    return(d)\n",
    "\n",
    "for t1,ts in second.items():\n",
    "    second[t1]=list2prob(ts)\n",
    "    \n",
    "for t,ts in transitions.items():\n",
    "    transitions[t]=list2prob(ts)   \n",
    "        \n",
    "def sample(d):\n",
    "    p0=np.random.random()\n",
    "    cum=0\n",
    "    for t,p in d.items():\n",
    "        cum+=p\n",
    "        if p0<cum:\n",
    "            return(t)\n",
    "    assert(False) #for testing purposes\n",
    "    \n",
    "def generate():\n",
    "    for i in range(4):\n",
    "        sent=[]\n",
    "        w0=sample(init)\n",
    "        sent.append(w0)\n",
    "        w1=sample(second[w0])\n",
    "        sent.append(w1)\n",
    "        while True:\n",
    "            w2=sample(transitions[(w0,w1)])\n",
    "            if w2=='END':\n",
    "                break\n",
    "            sent.append(w2)\n",
    "            w0,w1=w1,w2\n",
    "            \n",
    "    print(' '.join(sent))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now lefts no bigger than a harness gall\n"
     ]
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "Here there are 3 parts, the initial proababilities, the true transition probabilities, and the probability of observing one state given a true state. ($\\pi, A, B$).  We assume that the next state depends only on the previous state, and that what we observe depends only on the current state, not any other state or any other observation.  With markov models, we can train the model with  with only the probability of sequences, but with the hidden version, we also need the observation of hidden versus real state.  \n",
    "# Doubly embedded stochastic processes\n",
    "There are two stochastic processes, the inner, markov layer, and the outter observation layer.  The number of hidden states is a hyperparameter of the model, like in k-means.  It can be informed by knowledge of the system, if such knowledge exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward backward\n",
    "The probability of a sequence of observations (given knowledge of the HMM parameters) can be computed with the forward or backward algorithm.   \n",
    "$$\n",
    "P(x_1,...,x_T)=\\sum_{z_i=\\{1,...,M \\}, 1 \\le z \\le T} \\pi(z_1)P(x_1\\vert z_1)\\Pi_{t=2}^T P(z_t \\vert z_{t-1})P(x_t \\vert z_t) \n",
    "$$\n",
    "We can factor to make it simpler.  For the forward algorithm, define a new variable alpha to represent having observed a particular sequence x and being in a given state:\n",
    "$$\n",
    "\\alpha(t,i) :=P(x_1,\\dots,x_t,z_t=i)\n",
    "$$\n",
    "Then the algorithm has 3 steps.  For the initial value, $\\alpha(t,i)=\\pi_i B (i,x_t)$, and then, as an inductive step, assume that for $t \\le T$\n",
    "$$\n",
    "\\alpha(t+1,j)=\\sum_{i=1}^M \\alpha(t,i)A(i,j)B(j,x_{t+1})\n",
    "$$\n",
    "Then, finally, \n",
    "$$\n",
    "p(x) = \\sum_{i=1}^M \\alpha(T,i) = \\sum_{i=1}^M P(x_i, 1 \\le i \\le T \\vert z_i)\n",
    "$$\n",
    "For the backward algorithm, we start with defining $\\beta(t,i)=P(x_{t+1},\\dots,x_T \\vert z_t=i)$, then \n",
    "$\\beta(T,i)=1$ and the inductive step is \n",
    "$$\n",
    "\\beta(t,i)=\\sum_{j=1}^M A(i,j)B(j,x_{t+1})\\beta(t+1,j)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi algorithm\n",
    "If hdden states are modeled on something real, you might want to find the most likely sequence of hidden states, which can be done with the viterbi algorithm. so we introduce the variables \n",
    "$\\delta(t,i)=\\max \\{P(z_1,...z_t=i,x_1,...x_t \\}$, which is the maximum probability of ending up in state i at time t, and $\\psi(t,i)$ which is the time  sequence corresponding to the state sequence for which delta is maximized.\n",
    "Start with $\\delta(1,i)=\\pi_i B(i,x_1), \\psi(1,i)=0$.  \n",
    "Then the recursion step is \n",
    "$$\n",
    "\\delta(t,j)= \\max_i \\{ \\delta(t-1,i)A(i,j)\\}B(j,x_t),\\quad \\psi(t,j)=arg \\max_i \\{\\delta(t-1,i)A(i,j) \\}.  \n",
    "$$\n",
    "Finally, we terminate when we reach time T and find the maximum probability $p^* = \\max_i \\delta(T,i)$, the best last state $z(T)^* = arg \\max_i \\delta(T,i)$, and then all the previous best states $z(t)^*=\\psi(t+1,z(t+1)^*)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baum-Welch algorithm\n",
    "A method for training an HMM.   Uses the forward backward algorithm and expectation maximization.  Once we have computed alpha and beta from the forward-backward algorithm, we use the new variable phi, which is the probability of the actual states being a consecutive pair i,j given all observations $x_{i}$, $\\phi(t,i,j)=P(z_t=i,z_{t+1}=j \\,\\vert ,x_1...x_T)$.  It is calculated as \n",
    "$$\n",
    "\\phi(t,i,j)=\\frac{\\alpha(t,i)A(i,j)B(j,x_{t+1})\\beta(t+1,j)}{\\sum_i \\sum_j \\alpha(t,i)A(i,j)B(j,x_{t+1})\\beta(t+1,j)}\n",
    "$$\n",
    "Then sum phi over all second states to get $\\gamma(t,i)=\\sum_j \\phi(t,i,j)$ The sum of gamma over time gives the expected number of transitions from state i, while the sum of phi gives the expected number of transitions from state i to state j.  To get an entry for matrix A, divide the sum of phi by the sum of gamma, and to get B, sum gamma (for the states corresponding to the observed sequence) and divide by the sum of all gamma.\n",
    "The updates are derived by expectation maximization.  As an example of a problem, consider parts of speech and words.  The parts of speech are hidden states, with words as the observations.  The prediction problem of being given a sentence and identifying each part of speech is an application of the Viterbi algorithm.  In training data the POS tags are given, so we can use maximum liklihood to find the transition probabilities $A(i,j)$.  Likewise for the observation probabilities given the part of speech.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test case for code is a heads / tails coion flip\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "symbol_map = ['H', 'T']\n",
    "pi = np.array([0.5, 0.5]) #could start with either of two coind\n",
    "A = np.array([[0.1, 0.9], [0.8, 0.2]])  #switch back and forth between the coins\n",
    "B = np.array([[0.6, 0.4], [0.3, 0.7]]) #one coin is more likely to give heads and \n",
    "#the other to give tails.\n",
    "M, V = B.shape #M is the number of hidden states, and V is the vocabulary of \n",
    "#possible observations\n",
    "test=np.array([1,0])\n",
    "#print(A) #the numpy arrays are lists of rows\n",
    "#print(np.dot(test,A)) #left multiplication (row)\n",
    "#print(np.dot(A,test))#right multiplication (column)\n",
    "\n",
    "def generate_sequence(N):\n",
    "    s = np.random.choice(range(M), p=pi) # initial state\n",
    "    x = np.random.choice(range(V), p=B[s]) # initial observation\n",
    "    sequence = [x]\n",
    "    for n in range(N-1):\n",
    "        s = np.random.choice(range(M), p=A[s]) # next state\n",
    "        x = np.random.choice(range(V), p=B[s]) # next observation\n",
    "        sequence.append(x)\n",
    "    return( sequence)\n",
    "\n",
    "def main():\n",
    "    with open('coin_data.txt', 'w') as f:\n",
    "        for n in range(50): #generate 50 length 30 examples\n",
    "            sequence = generate_sequence(30)\n",
    "            sequence = ''.join(symbol_map[s] for s in sequence)\n",
    "            print(sequence)\n",
    "            f.write(\"%s\\n\" % sequence)\n",
    "            \n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[ 0.70301082  0.29698918]\n",
      " [ 0.28691526  0.71308474]]\n",
      "B: [[ 0.54590853  0.45409147]\n",
      " [ 0.52900905  0.47099095]]\n",
      "pi: [ 0.51343373  0.48656627]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD8CAYAAACCRVh7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fd7c72e048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LL with fitted params: -1035.54072226\n",
      "LL with true params: -1066.79859744\n",
      "Best state sequence for: [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0]\n",
      "[1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def random_normalized(d1, d2): # creates a random array with d1 rows d2 columns\n",
    "    #summing over the rows to normalize (so we left multiply state vectors)\n",
    "    x = np.random.random((d1, d2))\n",
    "    return x / x.sum(axis=1, keepdims=True)\n",
    "\n",
    "class HMM:\n",
    "    def __init__(self, M):\n",
    "        self.M = M # number of hidden states\n",
    "    def fit(self, X, max_iter=30):\n",
    "        np.random.seed(123)\n",
    "        # train the HMM model using the Baum-Welch algorithm\n",
    "        # a specific instance of the expectation-maximization algorithm\n",
    "\n",
    "        # determine V, the vocabulary size\n",
    "        # assume observables are already integers from 0..V-1\n",
    "        # X is a jagged array of observed sequences\n",
    "        V = max(max(x) for x in X) + 1 #get vocabury size\n",
    "        N = len(X)\n",
    "\n",
    "        self.pi = np.ones(self.M) / self.M # initial state distribution\n",
    "        self.A = random_normalized(self.M, self.M) # state transition matrix\n",
    "        self.B = random_normalized(self.M, V) # output distribution\n",
    "\n",
    "        costs = []\n",
    "        for it in range(max_iter):\n",
    "            alphas = [] \n",
    "            betas = []\n",
    "            P = np.zeros(N) #probabilities for each sequence\n",
    "            for n in range(N): #for each sequence\n",
    "                x = X[n] \n",
    "                T = len(x) \n",
    "                alpha = np.zeros((T, self.M)) #alpha stores all alpha values for\n",
    "                #each state for each element of the sequence\n",
    "                alpha[0] = self.pi*self.B[:,x[0]] #elementwise multiplication of initial\n",
    "                #state and the probability of observing x_0 given that state\n",
    "                for t in range(1, T): #for each observation in the sequence\n",
    "                    tmp1 = alpha[t-1].dot(self.A) * self.B[:, x[t]] #multiply the vector alpha\n",
    "                    #by the matrix A, then do element-wise multiplication with \n",
    "                    # the probability of observing x_t given that state\n",
    "                    # tmp2 = np.zeros(self.M)\n",
    "                    # for i in xrange(self.M):\n",
    "                    #     for j in xrange(self.M):\n",
    "                    #         tmp2[j] += alpha[t-1,i] * self.A[i,j] * self.B[j, x[t]]\n",
    "                    # print \"diff:\", np.abs(tmp1 - tmp2).sum()\n",
    "                    # assert(np.abs(tmp1 - tmp2).sum() < 10e-10)\n",
    "                    alpha[t] = tmp1\n",
    "                P[n] = alpha[-1].sum() #the probability of the sequence x is just the\n",
    "                #sum of the alphas, since each alpha is the probability of the sequence\n",
    "                #up to time t with the final state being j\n",
    "                alphas.append(alpha)\n",
    "\n",
    "                beta = np.zeros((T, self.M)) \n",
    "                beta[-1] = 1\n",
    "                #beta is for the backward method, so we count backward\n",
    "                for t in range(T - 2, -1, -1):\n",
    "                    beta[t] = self.A.dot(self.B[:, x[t+1]] * beta[t+1]) \n",
    "                    #beta_j is the probability of the future sequence occuring \n",
    "                    #given the present state is j, and beta is the vector of all\n",
    "                    #beta js.  Multiply the probability that the next observation is x_t+1\n",
    "                    # by the probability that the rest of the sequence follows, and multiply this\n",
    "                    #by the probability of the appropriate trainsition happenind (A)\n",
    "                betas.append(beta)\n",
    "\n",
    "            # print \"P:\", P\n",
    "            # break\n",
    "            assert(np.all(P > 0))\n",
    "            cost = np.sum(np.log(P)) #cost is log likelyhood of P\n",
    "            costs.append(cost)\n",
    "#after getting the probability of the sequences given A,B,pi,(the expectation step) \n",
    "#we now re-estimate A,B,pi. the maximization step. \n",
    "\n",
    "            # now re-estimate pi, A, B\n",
    "            self.pi = np.sum((alphas[n][0] * betas[n][0])/P[n] for n in range(N)) / N\n",
    "            # print \"self.pi:\", self.pi\n",
    "            # break\n",
    "            #den1 is the denominator for updating A, and den 2 for B\n",
    "\n",
    "            den1 = np.zeros((self.M, 1))\n",
    "            den2 = np.zeros((self.M, 1))\n",
    "            a_num = 0\n",
    "            b_num = 0\n",
    "            for n in range(N):\n",
    "                x = X[n]\n",
    "                T = len(x)\n",
    "                # print \"den shape:\", den.shape\n",
    "                # test = (alphas[n][:-1] * betas[n][:-1]).sum(axis=0, keepdims=True).T\n",
    "                # print \"shape (alphas[n][:-1] * betas[n][:-1]).sum(axis=0): \", test.shape\n",
    "                \n",
    "                den1 += (alphas[n][:-1] * betas[n][:-1]).sum(axis=0, keepdims=True).T / P[n]\n",
    "                den2 += (alphas[n] * betas[n]).sum(axis=0, keepdims=True).T / P[n]\n",
    "\n",
    "                # tmp2 = np.zeros((self.M, 1))\n",
    "                # for i in xrange(self.M):\n",
    "                #     for t in xrange(T-1):\n",
    "                #         tmp2[i] += alphas[n][t,i] * betas[n][t,i]\n",
    "                # tmp2 /= P[n]\n",
    "                # # print \"diff:\", np.abs(tmp1 - tmp2).sum()\n",
    "                # assert(np.abs(tmp1 - tmp2).sum() < 10e-10)\n",
    "                # den += tmp1\n",
    "\n",
    "                # numerator for A\n",
    "                a_num_n = np.zeros((self.M, self.M))\n",
    "                for i in range(self.M):\n",
    "                    for j in range(self.M):\n",
    "                        for t in range(T-1):\n",
    "                            a_num_n[i,j] += alphas[n][t,i] * self.A[i,j] * self.B[j, x[t+1]] * betas[n][t+1,j]\n",
    "                a_num += a_num_n / P[n]\n",
    "\n",
    "                # numerator for B\n",
    "                # b_num_n = np.zeros((self.M, V))\n",
    "                # for i in xrange(self.M):\n",
    "                #     for j in xrange(V):\n",
    "                #         for t in xrange(T):\n",
    "                #             if x[t] == j:\n",
    "                #                 b_num_n[i,j] += alphas[n][t][i] * betas[n][t][i]\n",
    "                b_num_n2 = np.zeros((self.M, V))\n",
    "                for i in range(self.M):\n",
    "                    for t in range(T):\n",
    "                        b_num_n2[i,x[t]] += alphas[n][t,i] * betas[n][t,i]\n",
    "                # assert(np.abs(b_num_n - b_num_n2).sum() < 10e-10)\n",
    "                b_num += b_num_n2 / P[n]\n",
    "            # tmp1 = a_num / den1\n",
    "            # tmp2 = np.zeros(a_num.shape)\n",
    "            # for i in xrange(self.M):\n",
    "            #     for j in xrange(self.M):\n",
    "            #         tmp2[i,j] = a_num[i,j] / den1[i]\n",
    "            # print \"diff:\", np.abs(tmp1 - tmp2).sum()\n",
    "            # print \"tmp1:\", tmp1\n",
    "            # print \"tmp2:\", tmp2\n",
    "            # assert(np.abs(tmp1 - tmp2).sum() < 10e-10)\n",
    "            self.A = a_num / den1\n",
    "            self.B = b_num / den2\n",
    "            # print \"P:\", P\n",
    "            # break\n",
    "        print(\"A:\", self.A)\n",
    "        print( \"B:\", self.B)\n",
    "        print( \"pi:\", self.pi)\n",
    "\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "        \n",
    "    def likelihood(self, x):\n",
    "        \n",
    "        # returns log P(x | model)\n",
    "        # using the forward part of the forward-backward algorithm\n",
    "        T = len(x)\n",
    "        alpha = np.zeros((T, self.M))\n",
    "        alpha[0] = self.pi*self.B[:,x[0]]\n",
    "        for t in range(1, T):\n",
    "            alpha[t] = alpha[t-1].dot(self.A) * self.B[:, x[t]]\n",
    "        return( alpha[-1].sum())\n",
    "\n",
    "    def likelihood_multi(self, X):\n",
    "        return( np.array([self.likelihood(x) for x in X]))\n",
    "\n",
    "    def log_likelihood_multi(self, X):\n",
    "        return( np.log(self.likelihood_multi(X)))\n",
    "\n",
    "    def get_state_sequence(self, x):\n",
    "        # returns the most likely state sequence given observed sequence x\n",
    "        # using the Viterbi algorithm\n",
    "        T = len(x)\n",
    "        delta = np.zeros((T, self.M))\n",
    "        psi = np.zeros((T, self.M))\n",
    "        delta[0] = self.pi*self.B[:,x[0]]\n",
    "        for t in range(1, T):\n",
    "            for j in range(self.M):\n",
    "                delta[t,j] = np.max(delta[t-1]*self.A[:,j]) * self.B[j, x[t]]\n",
    "                psi[t,j] = np.argmax(delta[t-1]*self.A[:,j])\n",
    "\n",
    "        # backtrack\n",
    "        states = np.zeros(T, dtype=np.int32)\n",
    "        states[T-1] = np.argmax(delta[T-1])\n",
    "        for t in range(T-2, -1, -1):\n",
    "            states[t] = psi[t+1, states[t+1]]\n",
    "        return( states)\n",
    "\n",
    "def fit_coin():\n",
    "    \n",
    "    X = []\n",
    "    for line in open('coin_data.txt'):\n",
    "        # 1 for H, 0 for T\n",
    "        x = [1 if e == 'H' else 0 for e in line.rstrip()]\n",
    "        X.append(x)\n",
    "\n",
    "    hmm = HMM(2)\n",
    "    hmm.fit(X)\n",
    "    L = hmm.log_likelihood_multi(X).sum()\n",
    "    print( \"LL with fitted params:\", L)\n",
    "    # try true values\n",
    "    hmm.pi = np.array([0.5, 0.5])\n",
    "    hmm.A = np.array([[0.1, 0.9], [0.8, 0.2]])\n",
    "    hmm.B = np.array([[0.6, 0.4], [0.3, 0.7]])\n",
    "    L = hmm.log_likelihood_multi(X).sum()\n",
    "    print( \"LL with true params:\", L)\n",
    "\n",
    "    # try viterbi\n",
    "    print( \"Best state sequence for:\", X[0])\n",
    "    print( hmm.get_state_sequence(X[0]))\n",
    "\n",
    "\n",
    "\n",
    "fit_coin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-53-6ca71a5a6d0e>, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-53-6ca71a5a6d0e>\"\u001b[1;36m, line \u001b[1;32m37\u001b[0m\n\u001b[1;33m    X = []\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
