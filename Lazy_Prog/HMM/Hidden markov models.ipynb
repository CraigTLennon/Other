{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The markov property\n",
    "A sequence on random variables has the markov property if the probability of the next value (state) depends only on the current state, and not on previous states given knowledge of the current state.  \n",
    "$$\n",
    "P( x_{i+1}\\, \\vert \\, x_i)=P( x_{i+1}\\, \\vert \\, x_j, \\forall \\, j\\le i )\n",
    "$$\n",
    "When there are a finite number of states, the transisiton probabilities can be represented by a transition martix $A(i,j)=P( x_{t}=j\\, \\vert \\, x_{t-1}=i)$.  With large state spaces, it may not be possible to get training data for every state transition.  Rather than setting the transition probabilities to zero, smoothing is used.  Without smoothing\n",
    "$$\n",
    "A(i,j)= \\frac{count(i \\rightarrow j)}{count(i)}\n",
    "$$\n",
    "With smoothing, we use $$\n",
    "A(i,j)= \\frac{count(i \\rightarrow j) +\\epsilon}{count(i)+\\epsilon N}\n",
    "$$\n",
    "Where N is the number of states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider a second order model for generating phrases trained on Robert Frost poetry.  In this model, we train to predict a word given the previous two words $$\n",
    "P(w_{t+1} \\, \\vert \\, w_t, w_{t-1})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "#for removing the punctuation from the poem\n",
    "def remove_pn(s):\n",
    "    translator=str.maketrans( '','',string.punctuation)\n",
    "    return([st.translate(translator) for st in s ])\n",
    "\n",
    "def add2dict(d,k,v):\n",
    "    if k not in d:\n",
    "        d[k]=[]\n",
    "    d[k].append(v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'l', 'l', '', ' ', 't', 'a', 'l', 'k']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tests\n",
    "s=\"all, talk\"\n",
    "remove_pn(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionaries for the first, second, and other words in sentences.\n",
    "init={}\n",
    "second={}\n",
    "transitions={}\n",
    "for line in open('robert_frost.txt'):\n",
    "    tokens=remove_pn(line.rstrip().lower().split())\n",
    "    N=len(tokens)\n",
    "    for i in range(N):\n",
    "        t=tokens[i]\n",
    "        if i==0:\n",
    "            init[t]=init.get(t,0)+1\n",
    "        else:\n",
    "            t1=tokens[i-1]\n",
    "            if i==N-1:\n",
    "                add2dict(transitions, (t1,t),'END')\n",
    "            if i ==1:\n",
    "                add2dict(second, t1,t)\n",
    "            else:\n",
    "                t2=tokens[i-2]\n",
    "                add2dict(transitions, (t2,t1),t)\n",
    "                \n",
    "#normlize distributions\n",
    "init_total=sum(init.values())\n",
    "for t,c in init.items():\n",
    "    init[t]=c/init_total\n",
    "    \n",
    "def list2prob(ts):\n",
    "    d={}\n",
    "    n=len(ts)\n",
    "    for t in ts:\n",
    "        d[t]=d.get(t,0)+1\n",
    "    for t,c, in d.items():\n",
    "        d[t]=c/n\n",
    "    return(d)\n",
    "\n",
    "for t1,ts in second.items():\n",
    "    second[t1]=list2prob(ts)\n",
    "    \n",
    "for t,ts in transitions.items():\n",
    "    transitions[t]=list2prob(ts)   \n",
    "        \n",
    "def sample(d):\n",
    "    p0=np.random.random()\n",
    "    cum=0\n",
    "    for t,p in d.items():\n",
    "        cum+=p\n",
    "        if p0<cum:\n",
    "            return(t)\n",
    "    assert(False) #for testing purposes\n",
    "    \n",
    "def generate():\n",
    "    for i in range(4):\n",
    "        sent=[]\n",
    "        w0=sample(init)\n",
    "        sent.append(w0)\n",
    "        w1=sample(second[w0])\n",
    "        sent.append(w1)\n",
    "        while True:\n",
    "            w2=sample(transitions[(w0,w1)])\n",
    "            if w2=='END':\n",
    "                break\n",
    "            sent.append(w2)\n",
    "            w0,w1=w1,w2\n",
    "            \n",
    "    print(' '.join(sent))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now lefts no bigger than a harness gall\n"
     ]
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
