{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The markov property\n",
    "A sequence on random variables has the markov property if the probability of the next value (state) depends only on the current state, and not on previous states given knowledge of the current state.  \n",
    "$$\n",
    "P( x_{i+1}\\, \\vert \\, x_i)=P( x_{i+1}\\, \\vert \\, x_j, \\forall \\, j\\le i )\n",
    "$$\n",
    "When there are a finite number of states, the transisiton probabilities can be represented by a transition martix $A(i,j)=P( x_{t}=j\\, \\vert \\, x_{t-1}=i)$.  With large state spaces, it may not be possible to get training data for every state transition.  Rather than setting the transition probabilities to zero, smoothing is used.  Without smoothing\n",
    "$$\n",
    "A(i,j)= \\frac{count(i \\rightarrow j)}{count(i)}\n",
    "$$\n",
    "With smoothing, we use $$\n",
    "A(i,j)= \\frac{count(i \\rightarrow j) +\\epsilon}{count(i)+\\epsilon N}\n",
    "$$\n",
    "Where N is the number of states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider a second order model for generating phrases trained on Robert Frost poetry.  In this model, we train to predict a word given the previous two words $$\n",
    "P(w_{t+1} \\, \\vert \\, w_t, w_{t-1})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "#for removing the punctuation from the poem\n",
    "def remove_pn(s):\n",
    "    translator=str.maketrans( '','',string.punctuation)\n",
    "    return([st.translate(translator) for st in s ])\n",
    "\n",
    "def add2dict(d,k,v):\n",
    "    if k not in d:\n",
    "        d[k]=[]\n",
    "    d[k].append(v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'l', 'l', '', ' ', 't', 'a', 'l', 'k']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tests\n",
    "s=\"all, talk\"\n",
    "remove_pn(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionaries for the first, second, and other words in sentences.\n",
    "init={}\n",
    "second={}\n",
    "transitions={}\n",
    "for line in open('robert_frost.txt'):\n",
    "    tokens=remove_pn(line.rstrip().lower().split())\n",
    "    N=len(tokens)\n",
    "    for i in range(N):\n",
    "        t=tokens[i]\n",
    "        if i==0:\n",
    "            init[t]=init.get(t,0)+1\n",
    "        else:\n",
    "            t1=tokens[i-1]\n",
    "            if i==N-1:\n",
    "                add2dict(transitions, (t1,t),'END')\n",
    "            if i ==1:\n",
    "                add2dict(second, t1,t)\n",
    "            else:\n",
    "                t2=tokens[i-2]\n",
    "                add2dict(transitions, (t2,t1),t)\n",
    "                \n",
    "#normlize distributions\n",
    "init_total=sum(init.values())\n",
    "for t,c in init.items():\n",
    "    init[t]=c/init_total\n",
    "    \n",
    "def list2prob(ts):\n",
    "    d={}\n",
    "    n=len(ts)\n",
    "    for t in ts:\n",
    "        d[t]=d.get(t,0)+1\n",
    "    for t,c, in d.items():\n",
    "        d[t]=c/n\n",
    "    return(d)\n",
    "\n",
    "for t1,ts in second.items():\n",
    "    second[t1]=list2prob(ts)\n",
    "    \n",
    "for t,ts in transitions.items():\n",
    "    transitions[t]=list2prob(ts)   \n",
    "        \n",
    "def sample(d):\n",
    "    p0=np.random.random()\n",
    "    cum=0\n",
    "    for t,p in d.items():\n",
    "        cum+=p\n",
    "        if p0<cum:\n",
    "            return(t)\n",
    "    assert(False) #for testing purposes\n",
    "    \n",
    "def generate():\n",
    "    for i in range(4):\n",
    "        sent=[]\n",
    "        w0=sample(init)\n",
    "        sent.append(w0)\n",
    "        w1=sample(second[w0])\n",
    "        sent.append(w1)\n",
    "        while True:\n",
    "            w2=sample(transitions[(w0,w1)])\n",
    "            if w2=='END':\n",
    "                break\n",
    "            sent.append(w2)\n",
    "            w0,w1=w1,w2\n",
    "            \n",
    "    print(' '.join(sent))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now lefts no bigger than a harness gall\n"
     ]
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "Here there are 3 parts, the initial proababilities, the true transition probabilities, and the probability of observing one state given a true state. ($\\pi, A, B$).  We assume that the next state depends only on the previous state, and that what we observe depends only on the current state, not any other state or any other observation.  With markov models, we can train the model with  with only the probability of sequences, but with the hidden version, we also need the observation of hidden versus real state.  \n",
    "# Doubly embedded stochastic processes\n",
    "There are two stochastic processes, the inner, markov layer, and the outter observation layer.  The number of hidden states is a hyperparameter of the model, like in k-means.  It can be informed by knowledge of the system, if such knowledge exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward backward\n",
    "The probability of a sequence is \n",
    "$$\n",
    "P(x_1,...,x_T)=\\sum_{z_i=\\{1,...,M \\}, 1 \\le z \\le T} \\pi(z_1)P(x_1\\vert z_1)\\Pi_{t=2}^T P(z_t \\vert z_{t-1})P(x_t \\vert z_t) \n",
    "$$\n",
    "We can factor to make it simpler.  For the forward algorithm, define a new variable alpha to represent having observed a particular sequence x and being in a given state:\n",
    "$$\n",
    "\\alpha(t,i) :=P(x_1,\\dots,x_t,z_t=i)\n",
    "$$\n",
    "Then the algorithm has 3 steps.  For the initial value, $\\alpha(t,i)=\\pi_i B (i,x_t)$, and then, as an inductive step, assume that for $t \\le T$\n",
    "$$\n",
    "\\alpha(t+1,j)=\\sum_{i=1}^M \\alpha(t,i)A(i,j)B(j,x_{t+1})\n",
    "$$\n",
    "Then, finally, \n",
    "$$\n",
    "p(x) = \\sum_{i=1}^M \\alpha(T,i) = \\sum_{i=1}^M P(x_i, 1 \\le i \\le T \\vert z_i)\n",
    "$$\n",
    "For the backward algorithm, we start with defining $\\beta(t,i)=P(x_{t+1},\\dots,x_T \\vert z_t=i)$, then \n",
    "$\\beta(T,i)=1$ and the inductive step is \n",
    "$$\n",
    "\\beta(t,i)=\\sum_{j=1}^M A(i,j)B(j,x_{t+1})\\beta(t+1,j)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
