{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From logistic regression to neural networks:\n",
    "\n",
    "letting \n",
    "$$\n",
    "a=x_1w_1+x_2w_2 \\qquad p(y|x)=\\frac{1}{1+e^{-a}}\n",
    "$$\n",
    "for the prediciton you can round to 1 or 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various types of nonlinearities:\n",
    "\n",
    "sigmoid(x): $$\n",
    "s(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "Tanh(x)\n",
    "$$\n",
    "t(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}\n",
    "$$\n",
    "relu(x)\n",
    "$$\n",
    "r(x)=\\max(x,0)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data, suppose there are N samples, and D dimensions so that we have an $N \\times D$ dimensional matrix X for the data, with $M$ nodes in the intermediate layer.  Then Z is an $N \\times M$ matrix, and $p(Y|X)$ is an $N \\times K$ vector for K classes.  The weights W must be \n",
    "$D \\times M$ and b is $M \\times 1$, and v is $M \\times 1$ with a scalar c (or k dim vector is there are k classes).\n",
    "$$\n",
    "Z = s(XW+B), \\qquad Y = s(Zv+C)\n",
    "$$\n",
    "where $B$ is $N$ stacked copies of $b^T$, since the bias must be added equally to each observation. (numpy does this automatically with x.dot(w)+b)  The same with C and the bias term c.\n",
    "On an observation and node level, each $$\n",
    "z_j=s \\left(\\sum_{i=1}^D w_{i,j}x_i +b_j \\right), \\qquad y_k=s \\left(\\sum_{j=1}^M v_{k,j}z_j+c_k \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax for k classes\n",
    "\n",
    "$p(y=k|X)1= e^{a_k}/Z, \\qquad Z = \\sum_i e^{a_i}, $ with W a $D \\times K$ matrix\n",
    "\n",
    "$$\n",
    "A_{N \\times K}=X_{N \\times D}W_{D \\times K} \\rightarrow Y_{N \\times K} = \\textrm{softmax} \\left(A_{N \\times K} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax and sigmoid are equivalent is you divide the softmax term by $\\exp(w_0^T x$ and assigne new weights $w=w_1-w_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
