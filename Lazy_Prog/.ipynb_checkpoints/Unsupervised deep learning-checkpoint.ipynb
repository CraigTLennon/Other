{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LZ prog unsupervised learning\n",
    "Principle Components Analysis is a change of basis and then projection.  The basis is chosen to maximize the variation captured by the new basis in a few vectors.  With the old data as x, the change of basis as Q, and the new data as   $z=Q^Tx$  When you combine all data into one matrix, X and Z are NxD, and $Z=xQ$  The features of this new space will be uncorrelated.  You only keep a few columns of Z, but still capture most of the variation.  The way to do it is to compute the covariance matrix of X:\n",
    "$$\n",
    "\\Sigma_X = \\frac{1}{N}(X-\\mu_X)^T(X-\\mu_X)\n",
    "$$\n",
    "a matrix in which cell i,j is the covariance of features i and j.  This matrix is symmetric, so will have a basis of orthogonal eigenvectors.  With the martix of eigenvalues as lambda, $\\Sigma_X V = V \\Lambda$ for the eigenvectors V, which is orthogonal, so the inverse is the transpose, and by replacing Z by XQ and computing its covariance, we find that $\\Sigma_Z=Q^T \\Sigma_X Q$.  Now letting the change of basis Q be V, we find that $\\Sigma_Z=V^T V  \\Lambda=\\Lambda$, so the new basis is uncorrelated and the eigenvalues are the variance of the new features.  We take as many as we need to get most of the variance.  While PCA is a decomposition based on the covariance of X, SVD is a decomposition of X as $X=USV^T$, where S is real and diagonal, and both U and V are self-conjugate ($V^T=V^{-1}$).  We use it by taking only the k vectors corresponding to the k largest diagonal elements of S, to approximate X by $\\hat X = U_kS_kV_k^T$, which is still NxD.\n",
    "\n",
    "t-SNE is a t-distribution based stochastic nonlinear embedding.  T-SNE tries to preserve distances between input vectors.  For symmetric SNE, you create a probability distribution, where \n",
    "$$\n",
    "p_{i,j} = \\frac{\\exp (-|x_i-x_j|^2 / 2\\sigma^2) }{\\sum_{k \\neq l }\\exp (-|x_k-x_l|^2 / 2\\sigma^2}\n",
    "$$\n",
    "And randomly initialize a low dimensional mapping to a space of dimension k< d, where Y is the NxK mapping matrix.\n",
    "$$\n",
    "q_{i,j} = \\frac{\\exp (-|y_i-y_j|^2 2) }{\\sum_{k \\neq l }\\exp (-|y_k-y_l|^2  }\n",
    "$$\n",
    "We want to make the two distributions p and q close, so minimize Kullbak-Leibler divergence (KL):\n",
    "$$\n",
    "C=KL(P||Q)=\\sum_i \\sum_j p_{i,j} \\log \\frac{p_{i,j}}{q_{i,j}}\n",
    "$$\n",
    "Then we minimize the cost by updating y, for example by gradient descent.  t-SNE is a modification intended to allow gaps between natural clusters, by using a t-distribution rather than guassian\n",
    "$$\n",
    "q_{i,j} = \\frac{(1+|y_i-y_j|^2)^{-1} }{\\sum_{k \\neq l }(1+|y_k-y_l|^2)^{-1}  }\n",
    "$$\n",
    "\n",
    "Autoencoders\n",
    "\n",
    "Autoencoders are trained to predict x as a target from x as an input.  They could use cross entropy or some other error function with a hidden layer which reduces the amount of data.  The input x is encoded as z, and then decoded as x'.  There is an encoder network and a decoder network.  Both z and x' (normalized) are produced by sigmoid layers.\n",
    "$$\n",
    "z=\\sigma( W_hx+b_h); \\qquad z=\\sigma( W_ox+b_o)\n",
    "$$\n",
    "sometimes weight sharing is used, and $W_h=W^T, W=W_o$.  These can be made denoising by adding noise to the input, but not output.  One can also stack autoencoders to make the input of one the output of a previous layer.  Comparing cross entropy and KL divergence, cross entropy is the epected message length when a different distribution q is assumed rather than the true distribuiton p, while KL measures how different one distribution is from another.\n",
    "$$\n",
    "KL(P||Q)=\\sum_ip_{i} \\log \\frac{p_{i}}{q_{i}}; \\qquad CE = E_p[-\\log Q] = -\\sum_i p_i \\log q_i; \\qquad KL=CE-E\n",
    "$$\n",
    "Restricted boltzman machines, rbms have a visible layer and a hidden layer, V,H and you can calculate h from v, or v from h. They are two way neural networks.  Bernoulli rbms have units which can only take on the values of zero of one.  In fact, you use the same weights for going back and forth.  With the probabilities below as vectors\n",
    "$$\n",
    "p(h=1 |v) = \\sigma (W^Tv+c); \\qquad p(v=1 |h) = \\sigma (W h+c)\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
