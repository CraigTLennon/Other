{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From LZ prog\n",
    "Sampling from a learned model.  For our initial example we can use a bayes classifier, a generative model in which for a class y, we model p(x|y) instead of trying to directly model p(y|x).  One method would be to find all the x belonging to class y, and then, model with, for example a normal rv for each class\n",
    "$$\n",
    "p(x|y)= N(\\mu_y,\\Sigma_y)\n",
    "$$\n",
    "We can then sample in one of two ways.  First, we can pick a class y and then sample from the appropriate Guassian.  Alternatively, we can choose y according to its distribution (number of class examples / total examples) and then sample from x given y, which means we are sampling from the joint distribution:\n",
    "$$\n",
    "p(y)p(x|y) = p(x,y)\n",
    "$$\n",
    "One can extend this by considering guassian mixture models, in which there is a latent variable z such that each category corresponds to some number or mixtures of z and each z is gaussian.  To sample with a GMM, sample y, then z given y, then x given z and y.  Variational inference is a baysian version of expectation maximization.  The variational inference version of the GMM contains an infinite number of clusters.\n",
    "\n",
    "Variational Autoencoders\n",
    "The VAE is and NN that learns to reproduce input through representation in an intermediate latent space from which it can draw input.  The variational refers to variational inference.  EM is used  when we have a latent variable and cannot maximize the variable we want to sample from directly, and gives us a point estiamte of paremeters.  In VI, we estimate those parameter distributions instead of making point estimates.    From paper by Blei et al, start assuming a joint density of latent variables z and observations x, p(z,x)=p(z)p(x|z), where the latent variables govern the distribution of data.  Bayesian models draws the latent variables from a prior distribution z and relates them to observations through p(x|z), then computing p(z|x), the posterior distribution.  In MCMC, you sample from an ergodic MC witha stationary distribution mirroring the posterior.  In variational inference, you assume a family of approximate densities $\\cal Q$ over teh latent variables.  Then optimize by finding a family member minimizing KL divergence to teh exact posterior (which we do not know --right?):\n",
    "$$\n",
    "q_*(z)=\\arg \\min_{q \\in \\cal Q} KL(q(z) ||p(z|x)) \n",
    "$$\n",
    "\n",
    "\n",
    "With traditional autoencoders, you treat the learning like a feed forward NN.  With a VAE there is a difference at the end of the encoder.  The output of the encoder is a mean and variance, from which a gaussian can be derived.  For the decoder, we draw a sample from the gaussian, go through an NN, and get the output.\n",
    "Example, our encoder could have input of size 4, a hidden layer of size 3, and a latent layer of dimension 2, so we sample from q(z), a 2 dimensional gaussian.  To reduce the number of parameters, we use an axis aligned gaussian with one variance param per dim -- so 0 covariance.  In practice, this means the latent layer has double the specified size (b/c need mu and sigma).  To ensure sigma is positive, use softplus activation, log(1+exp(a)).  Q(z) is an approximation for p(z|x).  \n",
    "\n",
    "In Bayesian ML, you have prior and posterior predictive distributiona and samples.  With VAE, you pass in input to get q(z|x), sample to get z, reconstruct via the decoder to get p(hat x | x) which is the posterior predictive sample.  We can obtain a prior predictive sample by sampling from z ~ N(0,1) and decoding that.  In order for this to work, we need an appropriate cost function.  The cost function we use is the evidence based lower bound, which we want to maximize.\n",
    "$$\n",
    "E[\\log p(x|z)] -  KL(q(z|x) ||p(z))\n",
    "$$\n",
    "where p(z) is N(0,1).  The form above shows the cost function to be a sum of the expected log liklihood of the data (target) + the divergence between prior and posterior (regularization)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
