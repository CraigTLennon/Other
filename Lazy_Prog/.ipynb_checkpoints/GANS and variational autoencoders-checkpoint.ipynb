{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From LZ prog\n",
    "Sampling from a learned model.  For our initial example we can use a bayes classifier, a generative model in which for a class y, we model p(x|y) instead of trying to directly model p(y|x).  One method would be to find all the x belonging to class y, and then, model with, for example a normal rv for each class\n",
    "$$\n",
    "p(x|y)= N(\\mu_y,\\Sigma_y)\n",
    "$$\n",
    "We can then sample in one of two ways.  First, we can pick a class y and then sample from the appropriate Guassian.  Alternatively, we can choose y according to its distribution (number of class examples / total examples) and then sample from x given y, which means we are sampling from the joint distribution:\n",
    "$$\n",
    "p(y)p(x|y) = p(x,y)\n",
    "$$\n",
    "One can extend this by considering guassian mixture models, in which there is a latent variable z such that each category corresponds to some number or mixtures of z and each z is gaussian.  To sample with a GMM, sample y, then z given y, then x given z and y.  Variational inference is a baysian version of expectation maximization.  The variational inference version of the GMM contains an infinite number of clusters.\n",
    "\n",
    "Variational Autoencoders\n",
    "The VAE is and NN that learns to reproduce input through representation in an intermediate latent space from which it can draw input.  The variational refers to variational inference.  EM is used  when we have a latent variable and cannot maximize the variable we want to sample from directly, and gives us a point estiamte of paremeters.  In VI, we estimate those parameter distributions instead of making point estimates.    From paper by Blei et al, start assuming a joint density of latent variables z and observations x, p(z,x)=p(z)p(x|z), where the latent variables govern the distribution of data.  Bayesian models draws the latent variables from a prior distribution z and relates them to observations through p(x|z), then computing p(z|x), the posterior distribution.  In MCMC, you sample from an ergodic MC witha stationary distribution mirroring the posterior.  In variational inference, you assume a family of approximate densities $\\cal Q$ over teh latent variables.  Then optimize by finding a family member minimizing KL divergence to teh exact posterior (which we do not know --right?):\n",
    "$$\n",
    "q_*(z)=\\arg \\min_{q \\in \\cal Q} KL(q(z) ||p(z|x)) \n",
    "$$\n",
    "\n",
    "\n",
    "With traditional autoencoders, you treat the learning like a feed forward NN.  With a VAE there is a difference at the end of the encoder.  The output of the encoder is a mean and variance, from which a gaussian can be derived.  For the decoder, we draw a sample from the gaussian, go through an NN, and get the output.\n",
    "Example, our encoder could have input of size 4, a hidden layer of size 3, and a latent layer of dimension 2, so we sample from q(z), a 2 dimensional gaussian.  To reduce the number of parameters, we use an axis aligned gaussian with one variance param per dim -- so 0 covariance.  In practice, this means the latent layer has double the specified size (b/c need mu and sigma).  To ensure sigma is positive, use softplus activation, log(1+exp(a)).  Q(z) is an approximation for p(z|x).  \n",
    "\n",
    "In Bayesian ML, you have prior and posterior predictive distributiona and samples.  With VAE, you pass in input to get q(z|x), sample to get z, reconstruct via the decoder to get p(hat x | x) which is the posterior predictive sample.  We can obtain a prior predictive sample by sampling from z ~ N(0,1) and decoding that.  In order for this to work, we need an appropriate cost function.  The cost function we use is the evidence based lower bound, which we want to maximize.\n",
    "$$\n",
    "E[\\log p(x|z)] -  KL(q(z|x) ||p(z))\n",
    "$$\n",
    "where p(z) is N(0,1).  The form above shows the cost function to be a sum of the expected log liklihood of the data (target) + the divergence between prior and posterior (regularization).\n",
    "\n",
    "Generative adversarial networks -- generating realistic examples.  There is no cost function for sampling quality of complex data like images.  A GAN is a combination of a generator and a discriminator.  For a cost function, consider the discriminator, which is trying to apply two labels, and can use binary cross entropy. \n",
    "$$\n",
    "J=-[t \\log y + (1-t) \\log (1-y)\n",
    "$$\n",
    "where t=0 if the image is real and 0 ow, and y=d(x)=p(image real|image).  Alternatively, using x for real and hat x for fake, so that d(x) is P(image labeled real | image real) and d(hat x) = P(image labeled real | image fake)\n",
    "\n",
    "$$\n",
    "J=-[ \\log d(x) + \\log (1-d(\\hat x))]=\\log \\frac{1-d(\\hat x)}{d(x)}\n",
    "$$\n",
    "So thei discriminator should maximize d(x) and minimize d(hat x), while the generator maximizes d (hat x).  (https://medium.com/deep-math-machine-learning-ai/ch-14-general-adversarial-networks-gans-with-math-1318faf46b43)\n",
    "Aggregated over all data, J is an expectation, based on the distribution of x.  The discriminator wants to maximize, and the generator to minimize the same objective function.  The generator starts from a uniform distribution and generates an image based on that.  The structure of the GAN depends on the type of data, image, etc.  One example is the DCGAN, which is a deep convolutional GAN that uses batch normalization (which normalizes data at every layer adaptively).  This uses three convolutional layers with 0 or 1 fully connected layers, an adam optimizer, and leaky relu.  The generator moves from a 4x4 with 1024 depth in feature generators to double the height and width at the expense of the depth.  The discriminator moves in the opposite direction.\n",
    "\n",
    "Batch normalization is normalization at each level of an NN.  The normalization occurs just before going through the activation function, not directly on the data.  We subtract mean and divide by std dev and then scale it and shift it with two new parameters.  If standardization is good, then the NN learns that gamma and beta should be 0, or it learns a different shift and scale.  At test time, we use a global mean and variation collected from training for the scaling.\n",
    "\n",
    "Fractionally strided convolutions allow us to increase our image size.  with a stride of 1/2, the result will be twice the size of the original.  With fractionally strided convolution, you put a space between the input of the pixel image, embedding the image in a larger image, and then using a stride of 1.  In tensorflow, the fractional conv function is conv2d_transpose.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
